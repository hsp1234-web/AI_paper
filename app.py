
# -*- coding: utf-8 -*-
from fastapi import FastAPI, Request, HTTPException, File, UploadFile, Form, Depends, BackgroundTasks, status
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.exceptions import RequestValidationError
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any
import uvicorn
import os
import shutil
import re
from datetime import datetime, timezone
import traceback
import asyncio
import uuid
from concurrent.futures import ThreadPoolExecutor
import json
import logging # å¼•å…¥ logging æ¨¡çµ„
import sys # ç”¨æ–¼æ›´åš´æ ¼çš„å•Ÿå‹•éŒ¯èª¤è™•ç†

from pytubefix import YouTube
from pytubefix.exceptions import RegexMatchError, VideoUnavailable, PytubeFixError

import google.generativeai as genai
from google.api_core import exceptions as google_exceptions # Added import
import mimetypes # Added import

# --- é…ç½®æ—¥èªŒ (é‡è¦) ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- è¨­å®šå¸¸æ•¸å’Œç›®éŒ„ ---
TEMP_AUDIO_STORAGE_DIR = "/content/ai_paper_temp_audio"
GENERATED_REPORTS_DIR = "/content/ai_paper_generated_reports"
MAX_CONCURRENT_TASKS = 2 # æœ€å¤§ä¸¦è¡Œä»»å‹™æ•¸

global_api_key: Optional[str] = None
api_key_is_valid: bool = False # è¿½è¹¤ API é‡‘é‘°çš„æœ‰æ•ˆæ€§
tasks_db: Dict[str, Dict[str, Any]] = {}
executor = ThreadPoolExecutor(max_workers=MAX_CONCURRENT_TASKS)

app = FastAPI(title="AI_paper API v2.4 (ç©©å®šæ€§å„ªåŒ–ç‰ˆ)")

# --- è‡ªè¨‚ RequestValidationError ä¾‹å¤–è™•ç† (å¼·åŒ–éŒ¯èª¤æ—¥èªŒ) ---
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    error_details = exc.errors()
    logger.error(f"--- [ERROR_VALIDATION] Request to '{request.url}' failed Pydantic validation ---")
    logger.error(f"[ERROR_VALIDATION] Details: {json.dumps(error_details, indent=2, ensure_ascii=False)}")
    logger.error(f"--- [ERROR_VALIDATION] End of Pydantic validation error details ---")

    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content={"detail": error_details},
    )

# --- Jinja2 æ¨¡æ¿è¨­å®š ---
templates_main_dir = "templates"
report_content_template_str = """
<div id='report-title-display' class='report-main-title'>{{ report_title }} (ç”± {{ model_id }} ç”Ÿæˆ)</div>
{% if summary_data %}
<div id='report-summary' class='report-content'>
  <h3><i class='fas fa-lightbulb icon-accent'></i> é‡é»æ‘˜è¦</h3>
  {% if summary_data.intro_paragraph %}
    <p class='intro-paragraph'>{{ summary_data.intro_paragraph }}</p>
  {% endif %}
  {% if summary_data.items %}
    {% for item in summary_data.items %}
      <h3><strong>{{ loop.index }}. {{ item.subtitle }}</strong></h3>
      {% if item.details %}
        <ul>
          {% for detail in item.details %}
            <li>{{ detail }}</li>
          {% endfor %}
        </ul>
      {% endif %}
    {% endfor %}
  {% endif %}
  {% if summary_data.bilingual_append %}
    <p>{{ summary_data.bilingual_append }}</p>
  {% endif %}
</div>
{% endif %}

{% if transcript_paragraphs %}
<div id='report-transcript' class='report-content'>
  <h3><i class='fas fa-file-alt icon-accent'></i> é€å­—ç¨¿</h3>
  {% if transcript_paragraphs.bilingual_prepend %}
    <p>{{ transcript_paragraphs.bilingual_prepend }}</p>
  {% endif %}
  {% for p_item in transcript_paragraphs.paragraphs %}
    {% if p_item.is_speaker_line %}
      <p><strong>{{ p_item.speaker }}:</strong> {{ p_item.content }}</p>
    {% else %}
      <p>{{ p_item.content }}</p>
    {% endif %}
    {% if p_item.insert_hr_after %}
      <hr class='transcript-divider'>
    {% endif %}
  {% endfor %}
</div>
{% endif %}
"""
from jinja2 import Template
report_content_jinja_template = Template(report_content_template_str)

# --- Pydantic æ¨¡å‹å®šç¾© (æ·»åŠ äº†æ›´è©³ç´°çš„ Field é©—è­‰) ---
class ProcessUrlRequest(BaseModel):
    url: str = Field(..., pattern=r"^https?:\/\/(www\.)?(youtube\.com|youtu\.be)\/.*$", description="æœ‰æ•ˆçš„ YouTube ç¶²å€")

class GenerateReportRequest(BaseModel):
    source_type: str = Field(..., pattern="^(youtube|upload)$", description="ä¾†æºé¡å‹ï¼Œåªèƒ½æ˜¯ 'youtube' æˆ– 'upload'")
    source_path: str = Field(..., min_length=1, description="éŸ³è¨Šä¾†æºçš„æœ¬åœ°æª”æ¡ˆè·¯å¾‘")
    model_id: str = Field(..., min_length=1, description="ç”¨æ–¼ç”Ÿæˆå ±å‘Šçš„ AI æ¨¡å‹ ID")
    output_options: List[str] = Field(..., min_items=1, description="å ±å‘Šè¼¸å‡ºæ ¼å¼é¸é …ï¼Œä¾‹å¦‚ 'summary_tc', 'md', 'txt'")
    # custom_prompts ä»ç‚º Optionalï¼Œå‰ç«¯æœƒæ ¹æ“šé‚è¼¯åˆ¤æ–·æ˜¯å¦å‚³é
    custom_prompts: Optional[Dict[str, str]] = Field(None, description="è‡ªè¨‚æç¤ºè©ï¼Œéµç‚º 'summary_prompt' æˆ– 'transcript_prompt'")

class SetApiKeyRequest(BaseModel):
    api_key: str = Field(..., min_length=10, description="Google API é‡‘é‘°")

# --- FastAPI äº‹ä»¶è™•ç† ---
@app.on_event("startup")
async def startup_event():
    os.makedirs(TEMP_AUDIO_STORAGE_DIR, exist_ok=True)
    os.makedirs(GENERATED_REPORTS_DIR, exist_ok=True)
    logger.info(f"è‡¨æ™‚éŸ³è¨Šå„²å­˜ç›®éŒ„ '{TEMP_AUDIO_STORAGE_DIR}' å·²ç¢ºèªå­˜åœ¨ã€‚")
    logger.info(f"ç”Ÿæˆå ±å‘Šå„²å­˜ç›®éŒ„ '{GENERATED_REPORTS_DIR}' å·²ç¢ºèªå­˜åœ¨ã€‚")

    # --- Local Temporary Audio File Cleanup (User Requirement: Disabled) ---
    # As per user requirements (Task 3, Step 1), the automatic cleanup of
    # files in TEMP_AUDIO_STORAGE_DIR has been disabled.
    # The user intends to manage these files manually or via a separate process.
    #
    # Original cleanup logic (now commented out):
    # # æ¸…ç†èˆŠçš„è‡¨æ™‚éŸ³è¨Šæª”æ¡ˆ (ç°¡å–®ç¤ºä¾‹ï¼šæ¸…ç†ä¸€å¤©å‰çš„æª”æ¡ˆ)
    # # å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œå¯èƒ½éœ€è¦æ›´è¤‡é›œçš„æ¸…ç†ç­–ç•¥æˆ–é€éå¤–éƒ¨æ’ç¨‹ä»»å‹™ä¾†åŸ·è¡Œ
    # # æ³¨æ„ï¼šé€™è£¡åªæ¸…ç† TEMP_AUDIO_STORAGE_DIRï¼Œä¸æ¸…ç† GENERATED_REPORTS_DIR
    # for filename in os.listdir(TEMP_AUDIO_STORAGE_DIR):
    #     file_path = os.path.join(TEMP_AUDIO_STORAGE_DIR, filename)
    #     try:
    #         if os.path.isfile(file_path) and (datetime.now() - datetime.fromtimestamp(os.path.getmtime(file_path))).days > 1:
    #             os.remove(file_path)
    #             logger.info(f"å·²æ¸…ç†éæœŸè‡¨æ™‚æª”æ¡ˆ: {filename}")
    #     except Exception as e:
    #         logger.warning(f"æ¸…ç†è‡¨æ™‚æª”æ¡ˆ {filename} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


    global global_api_key, api_key_is_valid
    env_api_key = os.getenv("GOOGLE_API_KEY")
    if env_api_key:
        logger.info("[INFO] åœ¨ç’°å¢ƒè®Šæ•¸ä¸­æ‰¾åˆ° GOOGLE_API_KEYã€‚å˜—è©¦é©—è­‰...")
        try:
            genai.configure(api_key=env_api_key)
            # å˜—è©¦ä¸€å€‹è¼•é‡ç´šçš„ API å‘¼å«ä¾†å¯¦è³ªé©—è­‰é‡‘é‘°
            # å¦‚æœèƒ½æˆåŠŸåˆ—å‡ºæ¨¡å‹ï¼Œå‰‡é‡‘é‘°æ‡‰æœ‰æ•ˆ
            next(genai.list_models(), None) # å˜—è©¦è¿­ä»£ä¸€å€‹ä»¥è§¸ç™¼æ½›åœ¨éŒ¯èª¤æˆ–é©—è­‰æˆåŠŸ
            global_api_key = env_api_key
            api_key_is_valid = True
            logger.info("[SUCCESS] ç’°å¢ƒè®Šæ•¸ä¸­çš„ GOOGLE_API_KEY å·²è¨­å®šä¸¦é©—è­‰æˆåŠŸã€‚")
        except Exception as e_configure:
            logger.error(f"[ERROR] ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ä¸­çš„ GOOGLE_API_KEY é…ç½® genai æ™‚ç™¼ç”ŸéŒ¯èª¤: {e_configure}")
            api_key_is_valid = False
    else:
        logger.info("[INFO] æœªåœ¨ç’°å¢ƒè®Šæ•¸ä¸­æ‰¾åˆ° GOOGLE_API_KEYã€‚ç­‰å¾…ä½¿ç”¨è€…é€é API è¨­å®šã€‚")

@app.on_event("shutdown")
async def shutdown_event():
    executor.shutdown(wait=True)
    logger.info("[INFO] ThreadPoolExecutor å·²é—œé–‰ã€‚")

# --- éœæ…‹æª”æ¡ˆèˆ‡ä¸»æ¨¡æ¿ (å¼·åŒ–å•Ÿå‹•æª¢æŸ¥) ---
try:
    current_cwd = os.getcwd()
    logger.debug(f"FastAPI app.py å•Ÿå‹•æ™‚çš„ CWD: {current_cwd}")
    static_dir_path = os.path.join(current_cwd, "static")
    templates_dir_path = os.path.join(current_cwd, templates_main_dir)

    # åš´æ ¼æª¢æŸ¥ç›®éŒ„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡çµ‚æ­¢æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•
    if not os.path.exists(static_dir_path):
        logger.critical(f"éœæ…‹æª”æ¡ˆç›®éŒ„ '{static_dir_path}' ä¸å­˜åœ¨ã€‚æ‡‰ç”¨ç¨‹å¼ç„¡æ³•å•Ÿå‹•ã€‚")
        sys.exit(1) # å¼·åˆ¶é€€å‡º
    if not os.path.exists(templates_dir_path):
        logger.critical(f"ä¸»æ¨¡æ¿ç›®éŒ„ '{templates_dir_path}' ä¸å­˜åœ¨ã€‚æ‡‰ç”¨ç¨‹å¼ç„¡æ³•å•Ÿå‹•ã€‚")
        sys.exit(1) # å¼·åˆ¶é€€å‡º

    app.mount("/static", StaticFiles(directory=static_dir_path), name="static")
    templates = Jinja2Templates(directory=templates_dir_path)
except Exception as e:
    logger.critical(f"è¨­å®šéœæ…‹æª”æ¡ˆæˆ–ä¸»æ¨¡æ¿æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}ã€‚æ‡‰ç”¨ç¨‹å¼ç„¡æ³•å•Ÿå‹•ã€‚")
    sys.exit(1) # å¼·åˆ¶é€€å‡º


# --- API é‡‘é‘°ç®¡ç† API (å¼·åŒ–é©—è­‰é‚è¼¯) ---
@app.post("/api/set_api_key")
async def set_api_key(request_data: SetApiKeyRequest):
    global global_api_key, api_key_is_valid
    logger.debug(f"æ¥æ”¶åˆ°è¨­å®š API é‡‘é‘°è«‹æ±‚ã€‚é‡‘é‘° (é®ç½©å¾Œ): {'*' * (len(request_data.api_key) - 4) + request_data.api_key[-4:] if len(request_data.api_key) > 4 else '***'}...")
    is_successfully_validated = False
    try:
        genai.configure(api_key=request_data.api_key)
        # å¯¦éš›é©—è­‰ï¼šå˜—è©¦åˆ—å‡ºæ¨¡å‹ï¼Œå¦‚æœå¤±æ•—å‰‡é‡‘é‘°å¯èƒ½ç„¡æ•ˆæˆ–æ¬Šé™ä¸è¶³
        next(genai.list_models(), None) # å˜—è©¦è¿­ä»£ä¸€å€‹ä»¥è§¸ç™¼æ½›åœ¨éŒ¯èª¤
        is_successfully_validated = True
    except Exception as e_val:
        logger.error(f"[ERROR] é©—è­‰æä¾›çš„ API é‡‘é‘°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e_val}")
        is_successfully_validated = False

    if is_successfully_validated:
        global_api_key = request_data.api_key
        api_key_is_valid = True
        logger.info(f"[SUCCESS] è‡¨æ™‚ API é‡‘é‘°å·²è¨­å®šä¸¦é©—è­‰æˆåŠŸã€‚")
        return {"message": "API é‡‘é‘°å·²è¨­å®šä¸¦é©—è­‰æˆåŠŸã€‚"}
    else:
        api_key_is_valid = False # ç¢ºä¿æ¨™è¨˜ç‚ºç„¡æ•ˆ
        logger.error(f"[ERROR] æä¾›çš„è‡¨æ™‚ API é‡‘é‘°é©—è­‰å¤±æ•—ã€‚")
        raise HTTPException(status_code=400, detail="æä¾›çš„ API é‡‘é‘°é©—è­‰å¤±æ•—ã€‚è«‹æª¢æŸ¥é‡‘é‘°æ˜¯å¦æ­£ç¢ºä¸”å…·æœ‰æ‰€éœ€æ¬Šé™ã€‚")

@app.get("/api/check_api_key_status")
async def check_api_key_status():
    global global_api_key, api_key_is_valid
    if global_api_key and api_key_is_valid:
        logger.debug("API é‡‘é‘°ç‹€æ…‹æª¢æŸ¥ï¼šå·²è¨­å®šä¸”æœ‰æ•ˆã€‚")
        return {"status": "set_and_valid", "message": "API é‡‘é‘°å·²è¨­å®šä¸”æœ‰æ•ˆã€‚"}
    elif global_api_key and not api_key_is_valid:
        logger.debug("API é‡‘é‘°ç‹€æ…‹æª¢æŸ¥ï¼šå·²è¨­å®šä½†é©—è­‰å¤±æ•—ã€‚")
        return {"status": "set_but_invalid", "message": "API é‡‘é‘°å·²è¨­å®šä½†é©—è­‰å¤±æ•—ï¼Œè«‹é‡æ–°è¨­å®šã€‚"}
    else:
        logger.debug("API é‡‘é‘°ç‹€æ…‹æª¢æŸ¥ï¼šæœªè¨­å®šã€‚")
        return {"status": "not_set", "message": "API é‡‘é‘°å°šæœªè¨­å®šï¼Œè«‹è¨­å®š API é‡‘é‘°ã€‚"}


# Helper function to upload audio file to Gemini Files API
def upload_audio_to_gemini_files(local_file_path: str, task_id_for_log: Optional[str] = "N/A") -> Optional[str]:
    """
    Uploads a local audio file to the Gemini Files API.

    Args:
        local_file_path: The path to the local audio file.
        task_id_for_log: Optional task ID for logging purposes.

    Returns:
        The Gemini File API resource name (e.g., "files/xxxxxxxx") if successful, None otherwise.
    """
    logger.info(f"[TASK {task_id_for_log}] Starting upload of {local_file_path} to Gemini Files API.")
    try:
        inferred_mime_type, _ = mimetypes.guess_type(local_file_path)
        upload_mime_type = inferred_mime_type

        if inferred_mime_type in ['audio/m4a', 'audio/mp4']: # m4a and mp4 (aac) are common for YouTube downloads
            upload_mime_type = 'audio/aac'
            logger.info(f"[TASK {task_id_for_log}] Original MIME type {inferred_mime_type} mapped to {upload_mime_type}.")
        elif not inferred_mime_type:
            upload_mime_type = 'application/octet-stream' # Default if type can't be inferred
            logger.warning(f"[TASK {task_id_for_log}] Could not infer MIME type for {local_file_path}. Defaulting to {upload_mime_type}.")
        else:
            logger.info(f"[TASK {task_id_for_log}] Inferred MIME type for {local_file_path}: {inferred_mime_type}. Uploading as {upload_mime_type}.")

        # Synchronous call, as this helper is expected to be run in a thread by the calling task.
        uploaded_file = genai.upload_file(path=local_file_path, mime_type=upload_mime_type)

        logger.info(f"[TASK {task_id_for_log}] Successfully uploaded {local_file_path} to Gemini Files API. File name: {uploaded_file.name}")
        return uploaded_file.name # Return the resource name like "files/xxxxxxxx"
    except google_exceptions.GoogleAPIError as e:
        logger.error(f"[TASK {task_id_for_log}] Gemini API error during file upload for {local_file_path}: {e}")
        return None
    except Exception as e:
        logger.error(f"[TASK {task_id_for_log}] An unexpected error occurred during file upload for {local_file_path}: {e}")
        return None


# --- è¼”åŠ©å‡½å¼ (sanitize_filename, generate_html_report_content_via_jinja) ---
def sanitize_base_filename(title: str, max_length: int = 50) -> str:
    if not title: title = "untitled_audio"
    title = str(title)
    title = re.sub(r'[\\/:*?"<>|&]', "_", title)
    title = title.replace(" ", "_")
    title = re.sub(r"_+", "_", title)
    title = title.strip('_')
    if title.startswith('.'): title = "_" + title[1:]
    return title[:max_length]

def generate_html_report_content_via_jinja(report_title: str, summary_data: Optional[Dict], transcript_data: Optional[Dict], model_id: str) -> str:
    try:
        return report_content_jinja_template.render(
            report_title=report_title,
            summary_data=summary_data,
            transcript_paragraphs=transcript_data,
            model_id=model_id
        )
    except Exception as e:
        logger.error(f"Jinja2 æ¨¡æ¿æ¸²æŸ“å ±å‘Šå…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        traceback.print_exc()
        return f"<div class='report-content'><p style='color:red;'>æŠ±æ­‰ï¼Œç”Ÿæˆå ±å‘Šé è¦½æ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤ï¼š{str(e)}</p></div>"

# --- process_audio_and_generate_report_task (å¼·åŒ–éŒ¯èª¤è™•ç†) ---
async def process_audio_and_generate_report_task(task_id: str, request_data: GenerateReportRequest):
    tasks_db[task_id]["status"] = "processing"
    tasks_db[task_id]["start_time"] = datetime.now(timezone.utc).isoformat()
    logger.info(f"[TASK {task_id}] è™•ç†é–‹å§‹: {request_data.source_path} (æ¨¡å‹: {request_data.model_id})")

    if not global_api_key or not api_key_is_valid:
        tasks_db[task_id]["status"] = "failed"
        tasks_db[task_id]["error_message"] = "API é‡‘é‘°ç„¡æ•ˆæˆ–æœªè¨­å®šæ–¼èƒŒæ™¯ä»»å‹™å•Ÿå‹•æ™‚ã€‚è«‹å…ˆè¨­å®šæœ‰æ•ˆçš„ API é‡‘é‘°ã€‚"
        tasks_db[task_id]["completion_time"] = datetime.now(timezone.utc).isoformat()
        logger.error(f"[TASK {task_id}] [ERROR] å¤±æ•— - API é‡‘é‘°ç„¡æ•ˆã€‚"); return

    # Upload the audio file to Gemini Files API
    # This is a synchronous call, but process_audio_and_generate_report_task is run in a ThreadPoolExecutor
    gemini_file_name = upload_audio_to_gemini_files(request_data.source_path, task_id)

    if gemini_file_name is None:
        logger.error(f"[TASK {task_id}] [ERROR] Failed to upload audio file to Gemini Files API. Aborting task.")
        tasks_db[task_id]["status"] = "failed"
        tasks_db[task_id]["error_message"] = "éŸ³è¨Šæª”æ¡ˆä¸Šå‚³è‡³ Gemini API å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒè™•ç†ã€‚"
        tasks_db[task_id]["completion_time"] = datetime.now(timezone.utc).isoformat()
        # As per user requirements (Task 3, Step 3), do not delete the local source file
        # even if the upload to Gemini Files API fails.
        # The user intends to manage local files manually or via a separate process.
        #
        # Original local file cleanup logic (now removed):
        # if os.path.exists(request_data.source_path):
        #     try:
        #         os.remove(request_data.source_path)
        #         logger.info(f"[TASK {task_id}] Cleaned up temporary file after failed upload: {request_data.source_path}")
        #     except OSError as e:
        #         logger.error(f"[TASK {task_id}] Error deleting temporary file {request_data.source_path} after failed upload: {e}")
        return

    logger.info(f"[TASK {task_id}] Gemini File Name (Resource Identifier): {gemini_file_name}")

    raw_response_text = None
    simulated_summary_text = None # Renaming for clarity, will hold extracted summary
    simulated_transcript_text = None # Renaming for clarity, will hold extracted transcript
    source_basename = os.path.basename(request_data.source_path) # Keep for report title

    try:
        logger.info(f"[TASK {task_id}] Initializing Gemini model: {request_data.model_id}")
        model = genai.GenerativeModel(request_data.model_id)

        logger.info(f"[TASK {task_id}] Retrieving File object for {gemini_file_name}")
        uploaded_file_reference = genai.get_file(name=gemini_file_name)
        # At this point, uploaded_file_reference.mime_type could be checked if needed,
        # but we trust the mime_type used during upload.

        content_for_api = [CORE_GEMINI_AUDIO_PROMPT_TW, uploaded_file_reference]

        logger.info(f"[TASK {task_id}] Sending request to Gemini API with prompt and audio file.")
        # Note: custom_prompts are explicitly not used here as per requirements.
        response = model.generate_content(content_for_api, stream=False)

        # Extract text - response.text should be used for non-streaming, non-function calling responses
        raw_response_text = response.text
        logger.info(f"[TASK {task_id}] Received raw response from Gemini API.")
        # logger.debug(f"[TASK {task_id}] Raw response: {raw_response_text[:500]}...") # Log a snippet

        # Parse the raw response text
        summary_match = re.search(r"\[é‡é»æ‘˜è¦é–‹å§‹\](.*?)\[é‡é»æ‘˜è¦çµæŸ\]", raw_response_text, re.DOTALL)
        transcript_match = re.search(r"\[è©³ç´°é€å­—ç¨¿é–‹å§‹\](.*?)\[è©³ç´°é€å­—ç¨¿çµæŸ\]", raw_response_text, re.DOTALL)

        if summary_match:
            simulated_summary_text = summary_match.group(1).strip()
            logger.info(f"[TASK {task_id}] Extracted summary from response.")
        else:
            logger.warning(f"[TASK {task_id}] Could not find summary markers in response. Summary will be empty.")
            simulated_summary_text = None # Ensure it's None if not found

        if transcript_match:
            simulated_transcript_text = transcript_match.group(1).strip()
            logger.info(f"[TASK {task_id}] Extracted transcript from response.")
        else:
            logger.warning(f"[TASK {task_id}] Could not find transcript markers in response. Transcript will be empty.")
            simulated_transcript_text = None # Ensure it's None if not found

        if not simulated_summary_text and not simulated_transcript_text:
            logger.error(f"[TASK {task_id}] Failed to extract both summary and transcript from Gemini response. Markers might be missing or response format incorrect.")
            # Consider this a failure if both are missing
            raise ValueError("é—œéµæ¨™è¨˜ (é‡é»æ‘˜è¦æˆ–è©³ç´°é€å­—ç¨¿) æœªåœ¨æ¨¡å‹å›æ‡‰ä¸­æ‰¾åˆ°ã€‚")

    except (google_exceptions.GoogleAPIError,
            google_exceptions.RetryError, # For network/retryable issues
            genai.types.BlockedPromptException, # If prompt is blocked
            genai.types.GenerationValidationException, # If response validation fails
            ValueError # For our custom value error above
            ) as e:
        error_message = f"å‘¼å« Gemini API æˆ–è™•ç†å…¶å›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {type(e).__name__} - {str(e)}"
        logger.error(f"[TASK {task_id}] [ERROR] {error_message}")
        tasks_db[task_id]["status"] = "failed"
        tasks_db[task_id]["error_message"] = error_message
        tasks_db[task_id]["completion_time"] = datetime.now(timezone.utc).isoformat()
        # No local file cleanup here, as it's handled by upload failure or outer try/finally for Gemini file
        return # Exit after setting error status
    except Exception as e: # Catch-all for other unexpected errors
        error_message = f"è™•ç†éŸ³è¨Šæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {type(e).__name__} - {str(e)}"
        logger.error(f"[TASK {task_id}] [ERROR] {error_message}")
        traceback.print_exc()
        tasks_db[task_id]["status"] = "failed"
        tasks_db[task_id]["error_message"] = error_message
        tasks_db[task_id]["completion_time"] = datetime.now(timezone.utc).isoformat()
        return # Exit after setting error status
    finally:
        # Clean up the file from Gemini Files API regardless of success or failure of generate_content
        if gemini_file_name:
            try:
                logger.info(f"[TASK {task_id}] Attempting to delete file {gemini_file_name} from Gemini Files API.")
                genai.delete_file(name=gemini_file_name)
                logger.info(f"[TASK {task_id}] Successfully deleted file {gemini_file_name} from Gemini Files API.")
            except Exception as e_delete:
                # Log error but don't let it crash the task or override original error
                logger.error(f"[TASK {task_id}] [ERROR] Failed to delete file {gemini_file_name} from Gemini Files API: {e_delete}")

    # If we've reached here and raw_response_text is None, it means an error handled by the `return` in except block.
    # This check is mostly a safeguard.
    if raw_response_text is None and tasks_db[task_id]["status"] != "failed":
        logger.error(f"[TASK {task_id}] [ERROR] Reached processing stage with no API response and no failure status. This should not happen.")
        tasks_db[task_id]["status"] = "failed"
        tasks_db[task_id]["error_message"] = "æœªçŸ¥çš„å…§éƒ¨éŒ¯èª¤ï¼Œç„¡æ³•ç²å– AI æ¨¡å‹å›æ‡‰ã€‚"
        tasks_db[task_id]["completion_time"] = datetime.now(timezone.utc).isoformat()
        return

    # Proceed with structuring data if summary or transcript text was found
    try:
        # è½‰æ›ç‚ºçµæ§‹åŒ–è³‡æ–™ (reusing existing logic with new variable names)
        structured_summary_data = None
        if simulated_summary_text: # Use the extracted summary
            lines = simulated_summary_text.strip().split('\n')
            intro = lines.pop(0) if lines else ""
            # bilingual_append_text is not relevant here as CORE_GEMINI_AUDIO_PROMPT_TW specifies TW Chinese.
            # if "transcript_bilingual_summary" in request_data.output_options and lines and "(This is the English part" in lines[-1]:
            #     bilingual_append_text = lines.pop(-1)
            bilingual_append_text = None # Explicitly set to None
            items = []
            current_item_details = []
            current_subtitle = None
            for line in lines:
                if line.startswith("**") and line.endswith("**"): # Assuming AI follows this for subtitles
                    if current_subtitle:
                        items.append({"subtitle": current_subtitle.strip('*'), "details": list(current_item_details)})
                    current_subtitle = line.strip('*')
                    current_item_details.clear()
                elif line.startswith("- ") and current_subtitle:
                    current_item_details.append(line[2:])
                elif current_subtitle and current_item_details: # Handle multi-line details
                    current_item_details[-1] += "\n" + line
                elif current_subtitle: # Handle details not starting with "-"
                    current_item_details.append(line)
                # If line doesn't fit above and there's no current_subtitle, it might be part of intro or ignored.
                # Consider if intro should capture more lines if no subtitles are present.
                # For now, this matches existing logic.

            if current_subtitle: # Add the last item
                items.append({"subtitle": current_subtitle.strip('*'), "details": list(current_item_details)})
            structured_summary_data = {"intro_paragraph": intro, "items": items, "bilingual_append": bilingual_append_text}

        structured_transcript_data = None
        if simulated_transcript_text: # Use the extracted transcript
            paragraphs_raw = simulated_transcript_text.strip().split('\n')
            hr_interval = 5 # This can be kept or removed if not desired for AI output
            formatted_paragraphs = []
            # bilingual_prepend_text is not relevant here.
            # if "transcript_bilingual_summary" in request_data.output_options and paragraphs_raw and paragraphs_raw[0].startswith("(Original Language Transcript"):
            #     bilingual_prepend_text = paragraphs_raw.pop(0)
            bilingual_prepend_text = None # Explicitly set to None
            for i, p_text in enumerate(paragraphs_raw):
                if p_text.strip(): # Ensure non-empty lines
                    # Updated regex to be more flexible with speaker labels (e.g., "ç™¼è¨€è€…A:", "Speaker Bï¼š", "æ—ç™½:")
                    match = re.match(r"^(ç™¼è¨€è€…\s?[A-Za-z0-9]+|Speaker\s?[A-Za-z0-9]+|æ—ç™½)[:ï¼š]\s*(.*)", p_text)
                    formatted_paragraphs.append({
                        "content": match.group(2).strip() if match else p_text.strip(),
                        "is_speaker_line": bool(match),
                        "speaker": match.group(1).strip() if match else None,
                        "insert_hr_after": (i + 1) % hr_interval == 0 and i < len(paragraphs_raw) - 1
                    })
            structured_transcript_data = {"bilingual_prepend": bilingual_prepend_text, "paragraphs": formatted_paragraphs}

        if not structured_summary_data and not structured_transcript_data and tasks_db[task_id]["status"] != "failed":
             # This case means parsing was successful but yielded no actual content from the AI's text.
            logger.warning(f"[TASK {task_id}] AI response was parsed, but no structured summary or transcript could be derived. The AI might have responded off-format.")
            # Depending on strictness, this could be a failure. For now, allow generating a report that indicates this.
            # tasks_db[task_id]["error_message"] = "AI å›æ‡‰æ ¼å¼æ­£ç¢ºï¼Œä½†æœªåŒ…å«æœ‰æ•ˆçš„æ‘˜è¦æˆ–é€å­—ç¨¿å…§å®¹ã€‚"
            # tasks_db[task_id]["status"] = "failed" # Optionally mark as failed
            # tasks_db[task_id]["completion_time"] = datetime.now(timezone.utc).isoformat()
            # return


        # æª”æ¡ˆç”Ÿæˆé‚è¼¯ (remains largely the same, uses structured_summary_data and structured_transcript_data)
        tasks_db[task_id]["status"] = "generating_report"
        # await asyncio.sleep(1) # Removed: No longer simulating report generation time, actual AI call was the delay

        report_title = f"'{source_basename}' çš„ AI åˆ†æå ±å‘Š (ç”± {request_data.model_id} ç”Ÿæˆ)" # Added model_id to title
        preview_html = generate_html_report_content_via_jinja(report_title, structured_summary_data, structured_transcript_data, request_data.model_id)

        base_report_filename = f"{sanitize_base_filename(source_basename, 30)}_{request_data.model_id}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        download_links = {}

        # ç”Ÿæˆå®Œæ•´çš„ HTML å ±å‘Š
        full_html_content = f"""<!DOCTYPE html><html lang="zh-Hant"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>{report_title}</title><link rel="stylesheet" href="/static/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"></head><body class="dark-mode"><div class="container content-wrapper" style="padding-top: 20px; padding-bottom: 20px;"><section class="card animated" style="margin-bottom:0;">{preview_html}</section></div></body></html>"""
        html_file_path = os.path.join(GENERATED_REPORTS_DIR, f"{base_report_filename}.html")
        try:
            with open(html_file_path, "w", encoding="utf-8") as f:
                f.write(full_html_content)
            download_links["html"] = f"/generated_reports/{os.path.basename(html_file_path)}"
        except Exception as e:
            logger.error(f"[TASK {task_id}] [ERROR] å„²å­˜ HTML å ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        # ç”Ÿæˆ Markdown å ±å‘Š
        if "md" in request_data.output_options:
            md_parts = [f"# {report_title}\n\n"]
            if structured_summary_data:
                md_parts.extend(
                    [f"## é‡é»æ‘˜è¦\n\n{structured_summary_data['intro_paragraph']}\n\n" if structured_summary_data.get('intro_paragraph') else "## é‡é»æ‘˜è¦\n\n"] +
                    [f"### {item['subtitle']}\n" + "".join([f"- {d}\n" for d in item.get('details', [])]) + "\n" for item in structured_summary_data.get('items', [])] +
                    ([f"{structured_summary_data['bilingual_append']}\n\n"] if structured_summary_data.get('bilingual_append') else [])
                )
            if structured_transcript_data:
                md_parts.extend(
                    ["## é€å­—ç¨¿\n\n"] +
                    ([f"{structured_transcript_data['bilingual_prepend']}\n\n"] if structured_transcript_data.get('bilingual_prepend') else []) +
                    [f"**{p['speaker']}:** {p['content']}\n\n" if p["is_speaker_line"] else f"{p['content']}\n\n" for p in structured_transcript_data.get('paragraphs', [])]
                )
            md_file_path = os.path.join(GENERATED_REPORTS_DIR, f"{base_report_filename}.md")
            try:
                with open(md_file_path, "w", encoding="utf-8") as f:
                    f.write("".join(md_parts))
                download_links["md"] = f"/generated_reports/{os.path.basename(md_file_path)}"
            except Exception as e:
                logger.error(f"[TASK {task_id}] [ERROR] å„²å­˜ Markdown å ±å‘ŠéŒ¯èª¤: {e}")

        # ç”Ÿæˆ TXT å ±å‘Š
        if "txt" in request_data.output_options:
            txt_parts = [f"{report_title}\n\n"]
            if structured_summary_data:
                txt_parts.extend(
                    ["é‡é»æ‘˜è¦\n--------------------\n"] +
                    ([f"{structured_summary_data['intro_paragraph']}\n\n"] if structured_summary_data.get('intro_paragraph') else []) +
                    [f"{item['subtitle']}\n" + "".join([f"  - {d}\n" for d in item.get('details', [])]) + "\n" for item in structured_summary_data.get('items', [])] +
                    ([f"{structured_summary_data['bilingual_append']}\n\n"] if structured_summary_data.get('bilingual_append') else [])
                )
            if structured_transcript_data:
                txt_parts.extend(
                    ["é€å­—ç¨¿\n--------------------\n"] +
                    ([f"{structured_transcript_data['bilingual_prepend']}\n\n"] if structured_transcript_data.get('bilingual_prepend') else []) +
                    [f"{p['speaker']}: {p['content']}\n\n" if p["is_speaker_line"] else f"{p['content']}\n\n" for p in structured_transcript_data.get('paragraphs', [])]
                )
            txt_file_path = os.path.join(GENERATED_REPORTS_DIR, f"{base_report_filename}.txt")
            try:
                with open(txt_file_path, "w", encoding="utf-8") as f:
                    f.write("".join(txt_parts))
                download_links["txt"] = f"/generated_reports/{os.path.basename(txt_file_path)}"
            except Exception as e:
                logger.error(f"[TASK {task_id}] [ERROR] å„²å­˜ TXT å ±å‘ŠéŒ¯èª¤: {e}")

        tasks_db[task_id]["status"] = "completed"
        tasks_db[task_id]["result_preview_html"] = preview_html
        tasks_db[task_id]["download_links"] = download_links
        tasks_db[task_id]["completion_time"] = datetime.now(timezone.utc).isoformat()
        logger.info(f"[TASK {task_id}] [SUCCESS] è™•ç†å®Œæˆã€‚")

    except Exception as e:
        tasks_db[task_id]["status"] = "failed"
        tasks_db[task_id]["error_message"] = f"å ±å‘Šè³‡æ–™çµæ§‹åŒ–æˆ–æª”æ¡ˆç”Ÿæˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {type(e).__name__} - {str(e)}"
        tasks_db[task_id]["completion_time"] = datetime.now(timezone.utc).isoformat()
        logger.error(f"[TASK {task_id}] [ERROR] å ±å‘Šè³‡æ–™çµæ§‹åŒ–æˆ–æª”æ¡ˆç”Ÿæˆæ™‚éŒ¯èª¤: {e}")
        traceback.print_exc()
        # Note: Gemini file cleanup is handled by the outer finally block.


# --- éŸ³è¨Šä¾†æºè™•ç† API ---
def _download_youtube_audio_sync(youtube_url: str, task_id_for_log: Optional[str]="N/A") -> str:
    logger.info(f"[TASK {task_id_for_log}] [SYNC_DOWNLOAD] é–‹å§‹ä¸‹è¼‰ YouTube éŸ³è¨Š: {youtube_url}")
    try:
        yt = YouTube(youtube_url)
        audio_stream = yt.streams.get_audio_only()
        if not audio_stream:
            audio_stream = yt.streams.filter(only_audio=True, file_extension='m4a').order_by('abr').desc().first()
        if not audio_stream:
            audio_stream = yt.streams.filter(only_audio=True, file_extension='webm').order_by('abr').desc().first()
        if not audio_stream:
            raise PytubeFixError(f"åœ¨ YouTube å½±ç‰‡ '{youtube_url}' ä¸­æ‰¾ä¸åˆ°åˆé©çš„éŸ³è¨Šæµã€‚")

        title_part = sanitize_base_filename(yt.title, max_length=30)
        timestamp_part = datetime.now().strftime("%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        file_extension = audio_stream.subtype if audio_stream.subtype else "mp4"
        final_filename = f"{title_part}_{timestamp_part}_{unique_id}.{file_extension}"
        actual_downloaded_path = audio_stream.download(output_path=TEMP_AUDIO_STORAGE_DIR, filename=final_filename)

        if not os.path.exists(actual_downloaded_path) or os.path.getsize(actual_downloaded_path) == 0:
            raise PytubeFixError(f"YouTube éŸ³è¨Šæª”æ¡ˆ '{final_filename}' ä¸‹è¼‰å¾Œæœªæ‰¾åˆ°æˆ–ç‚ºç©ºã€‚")

        logger.info(f"[TASK {task_id_for_log}] [SYNC_DOWNLOAD] [SUCCESS] YouTube éŸ³è¨ŠæˆåŠŸä¸‹è¼‰è‡³: {actual_downloaded_path}")
        return actual_downloaded_path
    except PytubeFixError as pte:
        error_msg = f"PytubeFix åœ¨è™•ç† YouTube éŸ³è¨Š '{youtube_url}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(pte)}"
        logger.error(f"[TASK {task_id_for_log}] [SYNC_DOWNLOAD] [ERROR] {error_msg}")
        traceback.print_exc()
        raise PytubeFixError(error_msg) from pte
    except Exception as e:
        error_msg = f"ä¸‹è¼‰ YouTube éŸ³è¨Š '{youtube_url}' æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {str(e)}"
        logger.error(f"[TASK {task_id_for_log}] [SYNC_DOWNLOAD] [ERROR] {error_msg}")
        traceback.print_exc()
        raise RuntimeError(error_msg) from e

@app.post("/api/process_youtube_url", response_model=Dict[str, str])
async def api_process_youtube_url(request_data: ProcessUrlRequest):
    log_task_id = str(uuid.uuid4())[:8]
    logger.info(f"[API_YT_URL] [TASK {log_task_id}] æ”¶åˆ° YouTube ç¶²å€è™•ç†è«‹æ±‚: {request_data.url}")
    # Pydantic å·²ç¶“åšäº† URL æ ¼å¼é©—è­‰ï¼Œé€™è£¡å¯ä»¥ç°¡åŒ–
    try:
        loop = asyncio.get_event_loop()
        local_audio_path = await loop.run_in_executor(executor, _download_youtube_audio_sync, request_data.url, log_task_id)
        return {"message": f"YouTube éŸ³è¨Š '{os.path.basename(local_audio_path)}' å·²æˆåŠŸä¸‹è¼‰è‡³ä¼ºæœå™¨ã€‚", "youtube_url": request_data.url, "processed_audio_path": local_audio_path}
    except PytubeFixError as pte:
        raise HTTPException(status_code=500, detail=str(pte))
    except Exception as e:
        logger.error(f"[API_YT_URL] [TASK {log_task_id}] [ERROR] è™•ç† YouTube ç¶²å€æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è™•ç† YouTube ç¶²å€æ™‚ç™¼ç”Ÿä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {str(e)}")

@app.post("/api/upload_audio_file", response_model=Dict[str, str])
async def api_upload_audio_file(audio_file: UploadFile = File(...)):
    log_task_id = str(uuid.uuid4())[:8]
    logger.info(f"[API_UPLOAD] [TASK {log_task_id}] æ”¶åˆ°æª”æ¡ˆä¸Šå‚³: {audio_file.filename}, é¡å‹: {audio_file.content_type}")
    original_name, original_ext = os.path.splitext(audio_file.filename if audio_file.filename else "uploaded_audio")

    # å˜—è©¦å¾ content_type æ¨æ–·å‰¯æª”å
    if not original_ext and audio_file.content_type:
        content_type_map = {
            "audio/mpeg": ".mp3", "audio/mp4": ".m4a", "audio/webm": ".webm",
            "audio/wav": ".wav", "audio/ogg": ".ogg", "audio/aac": ".aac"
        }
        original_ext = content_type_map.get(audio_file.content_type, ".bin") # é è¨­ç‚º .bin ä»¥é˜²è¬ä¸€

    base_name = sanitize_base_filename(original_name, max_length=30)
    timestamp = datetime.now().strftime("%m%d_%H%M%S")
    unique_id = str(uuid.uuid4())[:8]
    temp_upload_filename = f"{base_name}_{timestamp}_{unique_id}{original_ext}"
    temp_upload_path = os.path.join(TEMP_AUDIO_STORAGE_DIR, temp_upload_filename)

    try:
        # ä½¿ç”¨ asyncio.to_thread ç¢ºä¿æª”æ¡ˆå¯«å…¥ä¸æœƒé˜»å¡ FastAPI äº‹ä»¶å¾ªç’°
        await asyncio.to_thread(shutil.copyfileobj, audio_file.file, open(temp_upload_path, "wb"))
        logger.info(f"[API_UPLOAD] [TASK {log_task_id}] [SUCCESS] æª”æ¡ˆæˆåŠŸå„²å­˜è‡³: {temp_upload_path} (å¤§å°: {os.path.getsize(temp_upload_path)} bytes)")
        return {"message": f"éŸ³è¨Šæª”æ¡ˆ '{audio_file.filename}' ä¸Šå‚³ä¸¦å„²å­˜æˆåŠŸã€‚", "filename": audio_file.filename, "content_type": audio_file.content_type, "processed_audio_path": temp_upload_path}
    except Exception as e:
        logger.error(f"[API_UPLOAD] [TASK {log_task_id}] [ERROR] å„²å­˜ä¸Šå‚³æª”æ¡ˆå¤±æ•—: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è™•ç†ä¸Šå‚³æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    finally:
        await audio_file.close()


# --- AI æ¨¡å‹èˆ‡å ±å‘Šç”Ÿæˆ API ---

# é å®šç¾©æ¨¡å‹è³‡æ–™ (åŒ…å«ä¸­æ–‡ä»‹ç´¹å’Œæ’åºè³‡è¨Š)
PREDEFINED_MODELS_DATA_APP = {
    "models/gemini-1.5-flash-latest": {
        "id": "models/gemini-1.5-flash-latest",
        "dropdown_display_name": "âš¡ Gemini 1.5 Flash (æœ€æ–°ç‰ˆ)",
        "chinese_display_name": "Gemini 1.5 Flash æœ€æ–°ç‰ˆ",
        "chinese_summary_parenthesized": "ï¼ˆé€Ÿåº¦å¿«ã€å¤šåŠŸèƒ½ã€å¤šæ¨¡æ…‹ã€é©ç”¨æ–¼å¤šæ¨£åŒ–ä»»å‹™æ“´å±•ï¼‰",
        "chinese_input_output": "è¼¸å…¥ï¼šæ–‡å­—ã€éŸ³è¨Šã€åœ–ç‰‡ã€å½±ç‰‡ (è©³è¦‹å®˜æ–¹æ–‡ä»¶)ï¼›è¼¸å‡ºï¼šæ–‡å­—",
        "chinese_suitable_for": "éœ€è¦å¿«é€Ÿå›æ‡‰çš„å¤šæ¨£åŒ–ä»»å‹™ã€å¤§è¦æ¨¡æ‡‰ç”¨ã€èŠå¤©ã€æ‘˜è¦ã€éŸ³è¨Šè™•ç†ã€‚",
        "original_description_from_api": "Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.",
        "sort_priority": 1 # æ’åºå„ªå…ˆç´šï¼Œæ•¸å­—è¶Šå°è¶Šé å‰
    },
    "models/gemini-1.5-pro-latest": {
        "id": "models/gemini-1.5-pro-latest",
        "dropdown_display_name": "ğŸŒŸ Gemini 1.5 Pro (æœ€æ–°ç‰ˆ)",
        "chinese_display_name": "Gemini 1.5 Pro æœ€æ–°ç‰ˆ",
        "chinese_summary_parenthesized": "ï¼ˆåŠŸèƒ½å¼·å¤§ã€å¤§å‹ä¸Šä¸‹æ–‡ã€å¤šæ¨¡æ…‹ã€ç†è§£è¤‡é›œæƒ…å¢ƒï¼‰",
        "chinese_input_output": "è¼¸å…¥ï¼šæ–‡å­—ã€éŸ³è¨Šã€åœ–ç‰‡ã€å½±ç‰‡ (æœ€é«˜é”2ç™¾è¬tokenï¼Œè©³è¦‹å®˜æ–¹æ–‡ä»¶)ï¼›è¼¸å‡ºï¼šæ–‡å­—",
        "chinese_suitable_for": "è¤‡é›œæ¨ç†ã€é•·ç¯‡å…§å®¹ç†è§£èˆ‡ç”Ÿæˆã€å¤šæ¨¡æ…‹åˆ†æã€ç¨‹å¼ç¢¼ç”Ÿæˆèˆ‡è§£é‡‹ã€‚",
        "original_description_from_api": "Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.",
        "sort_priority": 0 # æœ€é«˜å„ªå…ˆç´š
    },
    "models/gemini-pro": { # å‡è¨­é€™æ˜¯ç´”æ–‡å­—çš„ gemini-pro
        "id": "models/gemini-pro",
        "dropdown_display_name": "Gemini Pro (ç´”æ–‡å­—)",
        "chinese_display_name": "Gemini Pro (ç´”æ–‡å­—ç‰ˆ)",
        "chinese_summary_parenthesized": "ï¼ˆå„ªåŒ–çš„ç´”æ–‡å­—ç”Ÿæˆèˆ‡ç†è§£ï¼‰",
        "chinese_input_output": "è¼¸å…¥ï¼šæ–‡å­—ï¼›è¼¸å‡ºï¼šæ–‡å­—",
        "chinese_suitable_for": "ç´”æ–‡å­—çš„å•ç­”ã€æ‘˜è¦ã€å¯«ä½œã€ç¿»è­¯ç­‰ã€‚",
        "original_description_from_api": "Optimized for text-only prompts.",
        "sort_priority": 10
    }
    # æ‚¨å¯ä»¥æ ¹æ“šéœ€è¦åŠ å…¥æ›´å¤šé å®šç¾©æ¨¡å‹å’Œå®ƒå€‘çš„ä¸­æ–‡è³‡è¨Š
}

CORE_GEMINI_AUDIO_PROMPT_TW = """
# è§’è‰²æ‰®æ¼”æŒ‡ä»¤
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é€å­—ç¨¿èˆ‡é‡é»æ‘˜è¦åˆ†æå¸«ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæˆ‘æä¾›çš„éŸ³è¨Šå…§å®¹ï¼Œç”¢å‡ºçµæ§‹åŒ–çš„ã€Œé‡é»æ‘˜è¦ã€èˆ‡ã€Œè©³ç´°é€å­—ç¨¿ã€ã€‚

# èªè¨€èˆ‡é¢¨æ ¼è¦æ±‚
è«‹å‹™å¿…ä½¿ç”¨**ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªç¿’æ…£ï¼‰**ä¾†æ›¸å¯«æ‰€æœ‰å…§å®¹ã€‚

# è¼¸å‡ºæ ¼å¼è¦æ±‚
ä½ å¿…é ˆåš´æ ¼ä¾ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼Œä¸å¯çœç•¥ä»»ä½•æ¨™è¨˜ï¼š

[é‡é»æ‘˜è¦é–‹å§‹]
(æ­¤è™•å¡«å¯«é‡é»æ‘˜è¦å…§å®¹ï¼Œè«‹æä¾›æ¢åˆ—å¼æˆ–åˆ†é»çš„æ‘˜è¦ï¼Œæ¯å€‹é‡é»å‰å¯ä»¥æœ‰å­æ¨™é¡Œ)
[é‡é»æ‘˜è¦çµæŸ]

---[é€å­—ç¨¿åˆ†éš”ç·š]---

[è©³ç´°é€å­—ç¨¿é–‹å§‹]
(æ­¤è™•å¡«å¯«è©³ç´°é€å­—ç¨¿å…§å®¹ï¼Œè‹¥èƒ½è­˜åˆ¥ä¸åŒç™¼è¨€è€…ï¼Œè«‹æ¨™è¨»ï¼Œä¾‹å¦‚ï¼šç™¼è¨€è€…Aï¼šå…§å®¹ã€‚)
[è©³ç´°é€å­—ç¨¿çµæŸ]
"""

def get_model_version_score(api_name_lower: str) -> int:
    score = 9999 # é è¨­è¼ƒä½å„ªå…ˆç´š
    if "latest" in api_name_lower: score = 0
    elif "preview" in api_name_lower:
        score = 1000
        date_match = re.search(r'preview[_-](\d{2})[_-]?(\d{2})', api_name_lower) # ä¾‹å¦‚ preview-0527
        if date_match:
            try:
                # çµ„åˆæ—¥æœŸï¼Œä¾‹å¦‚ 0527 -> 527ï¼Œæ•¸å­—è¶Šå¤§ä»£è¡¨æ—¥æœŸè¶Šæ–°
                date_score = int(date_match.group(1)) * 100 + int(date_match.group(2))
                score -= date_score # æ•¸å­—è¶Šå¤§è¶Šæ–°ï¼Œæ‰€ä»¥è¦æ¸›ï¼Œè®“ç¸½åˆ†æ•¸è®Šå°
            except ValueError:
                pass # è§£æå¤±æ•—å‰‡ä½¿ç”¨é è¨­å€¼
        else: score += 100 # ç„¡æ—¥æœŸçš„ preview æ¯”æœ‰æ—¥æœŸçš„èˆŠ
    elif "-exp" in api_name_lower or "experimental" in api_name_lower: score = 2000
    else: # å˜—è©¦è§£ææ•¸å­—ç‰ˆæœ¬è™Ÿï¼Œä¾‹å¦‚ -001
        num_version_match = re.search(r'-(\d+)$', api_name_lower.split('/')[-1])
        if num_version_match:
            try:
                score = 3000 - int(num_version_match.group(1)) # æ•¸å­—è¶Šå¤§ç‰ˆæœ¬è¶Šæ–°ï¼Œæ‰€ä»¥æ¸›
            except ValueError:
                score = 3500
    return score

def sort_models_key_function(model_dict: Dict[str, Any]):
    api_name = model_dict.get("id", "")
    name_lower = api_name.lower()

    # å„ªå…ˆç´šçµ„ (ä¾†è‡ªé å®šç¾©è³‡æ–™æˆ–æ ¹æ“šåç¨±çŒœæ¸¬)
    priority_group = model_dict.get("sort_priority", 99) # é å®šç¾©çš„å„ªå…ˆç´š
    if priority_group == 99: # å¦‚æœæ²’æœ‰é å®šç¾©çš„ï¼Œå˜—è©¦çŒœæ¸¬
        if "gemini-1.5-pro" in name_lower: priority_group = 2 # æ¯” flash latest ç¨ä½
        elif "gemini-1.5-flash" in name_lower: priority_group = 3
        elif "gemini-pro" in name_lower and "vision" not in name_lower : priority_group = 12
        elif "gemini" in name_lower: priority_group = 15
        else: priority_group = 20

    version_score = get_model_version_score(name_lower)

    # ä¸»è¦ç‰ˆæœ¬è™Ÿ (ä¾‹å¦‚ 1.5, 1.0) ç”¨æ–¼æ¬¡ç´šæ’åº
    main_version_num = 0.0
    main_version_match = re.search(r'(gemini(?:-1\.5)?)-(\d+\.\d+|\d+)', name_lower) # gemini-1.5-pro, gemini-pro
    if not main_version_match: main_version_match = re.search(r'gemini-(\d+\.\d+|\d+)', name_lower)

    if main_version_match:
        try:
            main_version_num = float(main_version_match.groups()[-1]) # å–æœ€å¾Œæ•ç²çš„æ•¸å­—çµ„
        except ValueError:
            pass

    # æ’åºè¦å‰‡ï¼šå„ªå…ˆç´šçµ„ (è¶Šå°è¶Šå„ªå…ˆ) -> ä¸»è¦ç‰ˆæœ¬è™Ÿ (è¶Šå¤§è¶Šå„ªå…ˆï¼Œæ‰€ä»¥å–è² å€¼) -> ç‰ˆæœ¬åˆ†æ•¸ (è¶Šå°è¶Šå„ªå…ˆ) -> å­—æ¯é †åº
    return (priority_group, -main_version_num, version_score, name_lower)


@app.get("/api/get_models")
async def api_get_models_enhanced():
    logger.info("[API_GET_MODELS_ENHANCED] è«‹æ±‚ç²å–å¢å¼·å‹ AI æ¨¡å‹åˆ—è¡¨ã€‚")

    if not api_key_is_valid: # API Key ç„¡æ•ˆæ™‚ä¸å˜—è©¦åˆ—å‡ºæ¨¡å‹
        logger.warning("[API_GET_MODELS_ENHANCED] API Key ç„¡æ•ˆï¼Œè¿”å›åŒ…å«éŒ¯èª¤è¨Šæ¯çš„æ¨¡å‹åˆ—è¡¨ã€‚")
        return JSONResponse(content=[
            {"id": "error-api-key-or-network",
             "dropdown_display_name": "éŒ¯èª¤ï¼šç„¡æ³•ç²å–æ¨¡å‹ (APIé‡‘é‘°ç„¡æ•ˆ)",
             "chinese_display_name": "APIé‡‘é‘°ç„¡æ•ˆæˆ–ç¶²è·¯å•é¡Œ",
             "chinese_summary_parenthesized": "ï¼ˆè«‹æª¢æŸ¥æ‚¨çš„ Google API é‡‘é‘°æ˜¯å¦æ­£ç¢ºä¸”æœ‰æ¬Šé™è¨ªå•æ¨¡å‹ï¼‰",
             "chinese_input_output": "N/A",
             "chinese_suitable_for": "è«‹åœ¨å·¦å´è¨­å®šå€è¼¸å…¥æœ‰æ•ˆçš„ API é‡‘é‘°ã€‚",
             "original_description_from_api": "API Key is invalid or not set. Failed to retrieve models from Google API.",
             "sort_priority": -1 # æœ€é«˜å„ªå…ˆç´šï¼Œç¢ºä¿åœ¨åˆ—è¡¨é ‚éƒ¨
            }
        ], status_code=500) # è¿”å› 500 éŒ¯èª¤ç‹€æ…‹ï¼Œè¡¨ç¤ºä¼ºæœå™¨ç«¯ç²å–æ¨¡å‹å¤±æ•—

    all_models_combined = {}
    try:
        logger.debug("[API_GET_MODELS_ENHANCED] æ­£åœ¨å¾ Google genai.list_models() æŸ¥è©¢ç·šä¸Šæ¨¡å‹...")
        online_models_count = 0
        for m_obj in genai.list_models():
            # ç¢ºä¿æ¨¡å‹æ”¯æ´ generateContent æ–¹æ³•
            if 'generateContent' in m_obj.supported_generation_methods:
                online_models_count +=1
                model_data = {
                    "id": m_obj.name,
                    "dropdown_display_name": m_obj.display_name if m_obj.display_name else m_obj.name.replace("models/", ""),
                    "chinese_display_name": m_obj.display_name if m_obj.display_name else m_obj.name.replace("models/", ""), # é è¨­ä½¿ç”¨ display_name
                    "chinese_summary_parenthesized": "", # é è¨­ç©º
                    "chinese_input_output": f"è¼¸å…¥ Tokens: {m_obj.input_token_limit}, è¼¸å‡º Tokens: {m_obj.output_token_limit}",
                    "chinese_suitable_for": "è«‹åƒè€ƒ API åŸå§‹æè¿°ã€‚",
                    "original_description_from_api": m_obj.description if m_obj.description else "N/A",
                    "sort_priority": 99 # é è¨­ä¸€å€‹è¼ƒä½çš„å„ªå…ˆç´šçµ¦ç·šä¸Šç²å–çš„ã€æœªåœ¨é å®šç¾©ä¸­å‡ºç¾çš„æ¨¡å‹
                }
                all_models_combined[m_obj.name] = model_data
        logger.debug(f"[API_GET_MODELS_ENHANCED] å¾ API ç²å–åˆ° {online_models_count} å€‹æ”¯æ´ generateContent çš„æ¨¡å‹ã€‚")

    except Exception as e_list_models:
        logger.error(f"[API_GET_MODELS_ENHANCED] å‘¼å« genai.list_models() å¤±æ•—: {e_list_models}")
        # å¦‚æœ API å‘¼å«å¤±æ•—ï¼Œè¿”å›æ˜ç¢ºçš„éŒ¯èª¤æ¨¡å‹ç‹€æ…‹çµ¦å‰ç«¯
        return JSONResponse(content=[
            {"id": "error-api-key-or-network",
             "dropdown_display_name": "éŒ¯èª¤ï¼šç„¡æ³•ç²å–æ¨¡å‹ (APIé‡‘é‘°æˆ–ç¶²è·¯å•é¡Œ)",
             "chinese_display_name": "APIé‡‘é‘°æˆ–ç¶²è·¯å•é¡Œ",
             "chinese_summary_parenthesized": "ï¼ˆç„¡æ³•é€£ç·šåˆ° Google APIï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ–é‡‘é‘°ç‹€æ…‹ï¼‰",
             "chinese_input_output": "N/A",
             "chinese_suitable_for": "è«‹æª¢æŸ¥æ‚¨çš„ API é‡‘é‘°æ˜¯å¦æ­£ç¢ºï¼Œä¸¦ç¢ºä¿ç¶²è·¯é€£æ¥æ­£å¸¸ã€‚",
             "original_description_from_api": "Failed to retrieve models from Google API. Check API key and network connectivity.",
             "sort_priority": -1 # æœ€é«˜å„ªå…ˆç´š
            }
        ], status_code=500) # è¿”å› 500 éŒ¯èª¤ç‹€æ…‹

    # å°‡é å®šç¾©çš„è³‡æ–™è¦†è“‹æˆ–æ·»åŠ åˆ°åˆä½µåˆ—è¡¨ä¸­ (é å®šç¾©çš„å„ªå…ˆ)
    for predefined_id, predefined_data in PREDEFINED_MODELS_DATA_APP.items():
        all_models_combined[predefined_id] = predefined_data # é å®šç¾©çš„è³‡æ–™æœ‰æ›´é«˜å„ªå…ˆç´š

    # è½‰æ›ç‚ºåˆ—è¡¨ä¸¦æ’åº
    sorted_models_list = sorted(all_models_combined.values(), key=sort_models_key_function)

    logger.info(f"[API_GET_MODELS_ENHANCED] è¿”å› {len(sorted_models_list)} å€‹æ¨¡å‹çµ¦å‰ç«¯ã€‚")
    return JSONResponse(content=sorted_models_list)


@app.post("/api/generate_report", status_code=202)
async def api_submit_generate_report_task(request_data: GenerateReportRequest, background_tasks: BackgroundTasks):
    task_id = str(uuid.uuid4())
    logger.info(f"[API_GEN_REPORT] [TASK {task_id}] æ”¶åˆ°å ±å‘Šç”Ÿæˆè«‹æ±‚: ä¾†æº='{request_data.source_path}', æ¨¡å‹='{request_data.model_id}'")

    if not global_api_key or not api_key_is_valid:
        logger.warning(f"[API_GEN_REPORT] [TASK {task_id}] API é‡‘é‘°ç„¡æ•ˆæˆ–æœªè¨­å®šã€‚")
        raise HTTPException(status_code=401, detail="ç„¡æ•ˆæˆ–æœªè¨­å®šçš„ API é‡‘é‘°ã€‚è«‹å…ˆè¨­å®šæœ‰æ•ˆçš„ API é‡‘é‘°ã€‚")

    if not os.path.exists(request_data.source_path):
        logger.error(f"[API_GEN_REPORT] [TASK {task_id}] éŸ³è¨Šä¾†æºæª”æ¡ˆä¸å­˜åœ¨: {request_data.source_path}")
        raise HTTPException(status_code=404, detail=f"æŒ‡å®šçš„éŸ³è¨Šä¾†æºæª”æ¡ˆä¸å­˜åœ¨: {os.path.basename(request_data.source_path)}")

    tasks_db[task_id] = {
        "task_id": task_id,
        "status": "queued",
        "source_name": os.path.basename(request_data.source_path),
        "model_id": request_data.model_id,
        "submit_time": datetime.now(timezone.utc).isoformat(),
        "start_time": None,
        "completion_time": None,
        "result_preview_html": None,
        "download_links": None,
        "error_message": None,
        "request_data": request_data.model_dump()
    }
    background_tasks.add_task(process_audio_and_generate_report_task, task_id, request_data)
    logger.info(f"[API_GEN_REPORT] [TASK {task_id}] ä»»å‹™å·²åŠ å…¥ä½‡åˆ—ã€‚")
    return {"task_id": task_id, "message": "å ±å‘Šç”Ÿæˆä»»å‹™å·²åŠ å…¥ä½‡åˆ—ã€‚", "status": "queued"}


# --- ä»»å‹™ç‹€æ…‹æŸ¥è©¢ API ---
@app.get("/api/tasks")
async def get_all_tasks_status():
    summary_tasks = [{
        "task_id": td["task_id"],
        "status": td["status"],
        "source_name": td["source_name"],
        "model_id": td["model_id"],
        "submit_time": td["submit_time"],
        "error_message": td.get("error_message"),
        "start_time": td.get("start_time"),
        "completion_time": td.get("completion_time"),
        "download_links": td.get("download_links")
    } for td in tasks_db.values()]
    return JSONResponse(content=sorted(summary_tasks, key=lambda x: x["submit_time"], reverse=True))

@app.get("/api/tasks/{task_id}")
async def get_task_status_and_result(task_id: str):
    logger.debug(f"[API_TASK_ID] è«‹æ±‚ç²å–ä»»å‹™ {task_id} çš„è©³ç´°ç‹€æ…‹ã€‚")
    task = tasks_db.get(task_id)
    if not task:
        logger.warning(f"[API_TASK_ID] æ‰¾ä¸åˆ°ä»»å‹™ ID: {task_id}")
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æŒ‡å®šçš„ä»»å‹™ IDã€‚")
    response_data = {
        "task_id": task["task_id"],
        "status": task["status"],
        "source_name": task["source_name"],
        "model_id": task["model_id"],
        "submit_time": task["submit_time"],
        "start_time": task.get("start_time"),
        "completion_time": task.get("completion_time"),
        "error_message": task.get("error_message")
    }
    if task["status"] == "completed":
        response_data["result_preview_html"] = task.get("result_preview_html")
        response_data["download_links"] = task.get("download_links")
    return JSONResponse(content=response_data)


# --- ä¸‹è¼‰ç”Ÿæˆçš„å ±å‘Šæª”æ¡ˆ API ---
try:
    if os.path.exists(GENERATED_REPORTS_DIR):
        app.mount("/generated_reports", StaticFiles(directory=GENERATED_REPORTS_DIR), name="generated_reports")
        logger.info(f"å·²æ›è¼‰ '{GENERATED_REPORTS_DIR}' è‡³ '/generated_reports' ä»¥ä¾›ç›´æ¥ä¸‹è¼‰ã€‚")
except Exception as e:
    logger.error(f"æ›è¼‰ generated_reports ç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# --- ä¸»é é¢ ---
@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    logger.debug("[API_ROOT] è«‹æ±‚ä¸»é é¢ã€‚")
    if templates is None:
        error_msg = "<html><body><h1>éŒ¯èª¤ï¼šä¸»æ¨¡æ¿å¼•æ“æœªåˆå§‹åŒ–ã€‚è«‹æª¢æŸ¥ä¼ºæœå™¨æ—¥èªŒã€‚</h1></body></html>"
        logger.critical("ä¸»æ¨¡æ¿å¼•æ“æœªåˆå§‹åŒ–ã€‚")
        return HTMLResponse(error_msg, status_code=500)
    return templates.TemplateResponse("index.html", {"request": request, "title": "AI_paper æ™ºèƒ½åŠ©ç† v2.4 (ç©©å®šç‰ˆ)"})


# --- API ç‹€æ…‹ ---
@app.get("/api/status")
async def get_api_status():
    logger.debug("[API_STATUS] è«‹æ±‚ API ç‹€æ…‹ã€‚")
    return {"status": "AI_paper API v2.4 is running", "version": "2.4.0_optimized"}

if __name__ == "__main__":
    logger.info("è‹¥åœ¨æœ¬åœ°åŸ·è¡Œ app.pyï¼Œè«‹ç¢ºä¿ CWD è¨­å®šæ­£ç¢ºã€‚")
    uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True, workers=1)


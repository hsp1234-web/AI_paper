
# -*- coding: utf-8 -*-
from fastapi import FastAPI, Request, HTTPException, File, UploadFile, Form, Depends, BackgroundTasks, status
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.exceptions import RequestValidationError
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any
import uvicorn
import os
import shutil
import re
from datetime import datetime, timezone
import traceback
import asyncio
import uuid
from concurrent.futures import ThreadPoolExecutor
import json
import logging # å¼•å…¥ logging æ¨¡çµ„
import sys # ç”¨æ–¼æ›´åš´æ ¼çš„å•Ÿå‹•éŒ¯èª¤è™•ç†

from pytubefix import YouTube
from pytubefix.exceptions import RegexMatchError, VideoUnavailable, PytubeFixError

import google.generativeai as genai

# --- Default Prompts ---
DEFAULT_SUMMARY_PROMPT_PY = "è«‹æä¾›é€™ä»½æ–‡ä»¶çš„ç¹é«”ä¸­æ–‡æ‘˜è¦ï¼Œé¢¨æ ¼æ‡‰å°ˆæ¥­ä¸”è³‡è¨Šè±å¯Œï¼ŒåŒ…å«ä¸€å€‹å¼•äººå…¥å‹çš„é–‹é ­æ®µè½ï¼Œæ¥è‘—åˆ—å‡º3-5å€‹å¸¶æœ‰å­æ¨™é¡Œçš„é—œéµé‡é»ï¼Œæ¯å€‹é‡é»æ‡‰æœ‰2-3å€‹è©³ç´°èªªæ˜ã€‚èªè¨€é¢¨æ ¼ï¼šå°ˆæ¥­ã€å­¸è¡“ã€å®¢è§€ã€‚"
DEFAULT_TRANSCRIPT_PROMPT_PY = "è«‹å°‡é€™ä»½éŸ³è¨Šé€å­—ç¨¿è½‰æ›æˆç¹é«”ä¸­æ–‡ï¼Œä¸¦ä¿®æ­£æ˜é¡¯çš„èªæ³•éŒ¯èª¤æˆ–å£èªè´…è©ï¼Œä½¿å…¶æ›´æµæš¢æ˜“è®€ã€‚å¦‚æœéŸ³æª”å…§å®¹åŒ…å«å¤šä½ç™¼è¨€è€…ï¼Œè«‹ç›¡å¯èƒ½æ¨™ç¤ºå‡ºä¸åŒçš„ç™¼è¨€è€… (ä¾‹å¦‚ï¼šç™¼è¨€è€…A, ç™¼è¨€è€…B)ã€‚"

# --- é…ç½®æ—¥èªŒ (é‡è¦) ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- è¨­å®šå¸¸æ•¸å’Œç›®éŒ„ ---
TEMP_AUDIO_STORAGE_DIR = "/content/ai_paper_temp_audio"
GENERATED_REPORTS_DIR = "/content/ai_paper_generated_reports"
MAX_CONCURRENT_TASKS = 2 # æœ€å¤§ä¸¦è¡Œä»»å‹™æ•¸

global_api_key: Optional[str] = None
api_key_is_valid: bool = False # è¿½è¹¤ API é‡‘é‘°çš„æœ‰æ•ˆæ€§
tasks_db: Dict[str, Dict[str, Any]] = {}
executor = ThreadPoolExecutor(max_workers=MAX_CONCURRENT_TASKS)

app = FastAPI(title="AI_paper API v2.4 (ç©©å®šæ€§å„ªåŒ–ç‰ˆ)")

# --- è‡ªè¨‚ RequestValidationError ä¾‹å¤–è™•ç† (å¼·åŒ–éŒ¯èª¤æ—¥èªŒ) ---
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    error_details = exc.errors()
    logger.error(f"--- [ERROR_VALIDATION] Request to '{request.url}' failed Pydantic validation ---")
    logger.error(f"[ERROR_VALIDATION] Details: {json.dumps(error_details, indent=2, ensure_ascii=False)}")
    logger.error(f"--- [ERROR_VALIDATION] End of Pydantic validation error details ---")

    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content={"detail": error_details},
    )

# --- Jinja2 æ¨¡æ¿è¨­å®š ---
templates_main_dir = "templates"
report_content_template_str = """
<div id='report-title-display' class='report-main-title'>{{ report_title }} (ç”± {{ model_id }} ç”Ÿæˆ)</div>
{% if summary_data %}
<div id='report-summary' class='report-content'>
  <h3><i class='fas fa-lightbulb icon-accent'></i> é‡é»æ‘˜è¦</h3>
  {% if summary_data.intro_paragraph %}
    <p class='intro-paragraph'>{{ summary_data.intro_paragraph }}</p>
  {% endif %}
  {% if summary_data.items %}
    {% for item in summary_data.items %}
      <h3><strong>{{ loop.index }}. {{ item.subtitle }}</strong></h3>
      {% if item.details %}
        <ul>
          {% for detail in item.details %}
            <li>{{ detail }}</li>
          {% endfor %}
        </ul>
      {% endif %}
    {% endfor %}
  {% endif %}
  {% if summary_data.bilingual_append %}
    <p>{{ summary_data.bilingual_append }}</p>
  {% endif %}
</div>
{% endif %}

{% if transcript_paragraphs %}
<div id='report-transcript' class='report-content'>
  <h3><i class='fas fa-file-alt icon-accent'></i> é€å­—ç¨¿</h3>
  {% if transcript_paragraphs.bilingual_prepend %}
    <p>{{ transcript_paragraphs.bilingual_prepend }}</p>
  {% endif %}
  {% for p_item in transcript_paragraphs.paragraphs %}
    {% if p_item.is_speaker_line %}
      <p><strong>{{ p_item.speaker }}:</strong> {{ p_item.content }}</p>
    {% else %}
      <p>{{ p_item.content }}</p>
    {% endif %}
    {% if p_item.insert_hr_after %}
      <hr class='transcript-divider'>
    {% endif %}
  {% endfor %}
</div>
{% endif %}
"""
from jinja2 import Template
report_content_jinja_template = Template(report_content_template_str)

# --- Pydantic æ¨¡å‹å®šç¾© (æ·»åŠ äº†æ›´è©³ç´°çš„ Field é©—è­‰) ---
class ProcessUrlRequest(BaseModel):
    url: str = Field(..., pattern=r"^https?:\/\/(www\.)?(youtube\.com|youtu\.be)\/.*$", description="æœ‰æ•ˆçš„ YouTube ç¶²å€")

class GenerateReportRequest(BaseModel):
    source_type: str = Field(..., pattern="^(youtube|upload)$", description="ä¾†æºé¡å‹ï¼Œåªèƒ½æ˜¯ 'youtube' æˆ– 'upload'")
    source_path: str = Field(..., min_length=1, description="éŸ³è¨Šä¾†æºçš„æœ¬åœ°æª”æ¡ˆè·¯å¾‘")
    model_id: str = Field(..., min_length=1, description="ç”¨æ–¼ç”Ÿæˆå ±å‘Šçš„ AI æ¨¡å‹ ID")
    output_options: List[str] = Field(..., min_items=1, description="å ±å‘Šè¼¸å‡ºæ ¼å¼é¸é …ï¼Œä¾‹å¦‚ 'summary_tc', 'md', 'txt'")
    # custom_prompts ä»ç‚º Optionalï¼Œå‰ç«¯æœƒæ ¹æ“šé‚è¼¯åˆ¤æ–·æ˜¯å¦å‚³é
    custom_prompts: Optional[Dict[str, str]] = Field(None, description="è‡ªè¨‚æç¤ºè©ï¼Œéµç‚º 'summary_prompt' æˆ– 'transcript_prompt'")

class SetApiKeyRequest(BaseModel):
    api_key: str = Field(..., min_length=10, description="Google API é‡‘é‘°")

# --- FastAPI äº‹ä»¶è™•ç† ---
@app.on_event("startup")
async def startup_event():
    os.makedirs(TEMP_AUDIO_STORAGE_DIR, exist_ok=True)
    os.makedirs(GENERATED_REPORTS_DIR, exist_ok=True)
    logger.info(f"è‡¨æ™‚éŸ³è¨Šå„²å­˜ç›®éŒ„ '{TEMP_AUDIO_STORAGE_DIR}' å·²ç¢ºèªå­˜åœ¨ã€‚")
    logger.info(f"ç”Ÿæˆå ±å‘Šå„²å­˜ç›®éŒ„ '{GENERATED_REPORTS_DIR}' å·²ç¢ºèªå­˜åœ¨ã€‚")

    # æ¸…ç†èˆŠçš„è‡¨æ™‚éŸ³è¨Šæª”æ¡ˆ (ç°¡å–®ç¤ºä¾‹ï¼šæ¸…ç†ä¸€å¤©å‰çš„æª”æ¡ˆ)
    # å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œå¯èƒ½éœ€è¦æ›´è¤‡é›œçš„æ¸…ç†ç­–ç•¥æˆ–é€éå¤–éƒ¨æ’ç¨‹ä»»å‹™ä¾†åŸ·è¡Œ
    # æ³¨æ„ï¼šé€™è£¡åªæ¸…ç† TEMP_AUDIO_STORAGE_DIRï¼Œä¸æ¸…ç† GENERATED_REPORTS_DIR
    for filename in os.listdir(TEMP_AUDIO_STORAGE_DIR):
        file_path = os.path.join(TEMP_AUDIO_STORAGE_DIR, filename)
        try:
            if os.path.isfile(file_path) and (datetime.now() - datetime.fromtimestamp(os.path.getmtime(file_path))).days > 1:
                os.remove(file_path)
                logger.info(f"å·²æ¸…ç†éæœŸè‡¨æ™‚æª”æ¡ˆ: {filename}")
        except Exception as e:
            logger.warning(f"æ¸…ç†è‡¨æ™‚æª”æ¡ˆ {filename} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


    global global_api_key, api_key_is_valid
    env_api_key = os.getenv("GOOGLE_API_KEY")
    if env_api_key:
        logger.info("[INFO] åœ¨ç’°å¢ƒè®Šæ•¸ä¸­æ‰¾åˆ° GOOGLE_API_KEYã€‚å˜—è©¦é©—è­‰...")
        try:
            genai.configure(api_key=env_api_key)
            # å˜—è©¦ä¸€å€‹è¼•é‡ç´šçš„ API å‘¼å«ä¾†å¯¦è³ªé©—è­‰é‡‘é‘°
            # å¦‚æœèƒ½æˆåŠŸåˆ—å‡ºæ¨¡å‹ï¼Œå‰‡é‡‘é‘°æ‡‰æœ‰æ•ˆ
            next(genai.list_models(), None) # å˜—è©¦è¿­ä»£ä¸€å€‹ä»¥è§¸ç™¼æ½›åœ¨éŒ¯èª¤æˆ–é©—è­‰æˆåŠŸ
            global_api_key = env_api_key
            api_key_is_valid = True
            logger.info("[SUCCESS] ç’°å¢ƒè®Šæ•¸ä¸­çš„ GOOGLE_API_KEY å·²è¨­å®šä¸¦é©—è­‰æˆåŠŸã€‚")
        except Exception as e_configure:
            logger.error(f"[ERROR] ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ä¸­çš„ GOOGLE_API_KEY é…ç½® genai æ™‚ç™¼ç”ŸéŒ¯èª¤: {e_configure}")
            api_key_is_valid = False
    else:
        logger.info("[INFO] æœªåœ¨ç’°å¢ƒè®Šæ•¸ä¸­æ‰¾åˆ° GOOGLE_API_KEYã€‚ç­‰å¾…ä½¿ç”¨è€…é€é API è¨­å®šã€‚")

@app.on_event("shutdown")
async def shutdown_event():
    executor.shutdown(wait=True)
    logger.info("[INFO] ThreadPoolExecutor å·²é—œé–‰ã€‚")

# --- éœæ…‹æª”æ¡ˆèˆ‡ä¸»æ¨¡æ¿ (å¼·åŒ–å•Ÿå‹•æª¢æŸ¥) ---
try:
    current_cwd = os.getcwd()
    logger.debug(f"FastAPI app.py å•Ÿå‹•æ™‚çš„ CWD: {current_cwd}")
    static_dir_path = os.path.join(current_cwd, "static")
    templates_dir_path = os.path.join(current_cwd, templates_main_dir)

    # åš´æ ¼æª¢æŸ¥ç›®éŒ„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡çµ‚æ­¢æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•
    if not os.path.exists(static_dir_path):
        logger.critical(f"éœæ…‹æª”æ¡ˆç›®éŒ„ '{static_dir_path}' ä¸å­˜åœ¨ã€‚æ‡‰ç”¨ç¨‹å¼ç„¡æ³•å•Ÿå‹•ã€‚")
        sys.exit(1) # å¼·åˆ¶é€€å‡º
    if not os.path.exists(templates_dir_path):
        logger.critical(f"ä¸»æ¨¡æ¿ç›®éŒ„ '{templates_dir_path}' ä¸å­˜åœ¨ã€‚æ‡‰ç”¨ç¨‹å¼ç„¡æ³•å•Ÿå‹•ã€‚")
        sys.exit(1) # å¼·åˆ¶é€€å‡º

    app.mount("/static", StaticFiles(directory=static_dir_path), name="static")
    templates = Jinja2Templates(directory=templates_dir_path)
except Exception as e:
    logger.critical(f"è¨­å®šéœæ…‹æª”æ¡ˆæˆ–ä¸»æ¨¡æ¿æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}ã€‚æ‡‰ç”¨ç¨‹å¼ç„¡æ³•å•Ÿå‹•ã€‚")
    sys.exit(1) # å¼·åˆ¶é€€å‡º


# --- API é‡‘é‘°ç®¡ç† API (å¼·åŒ–é©—è­‰é‚è¼¯) ---
@app.post("/api/set_api_key")
async def set_api_key(request_data: SetApiKeyRequest):
    global global_api_key, api_key_is_valid
    logger.debug(f"æ¥æ”¶åˆ°è¨­å®š API é‡‘é‘°è«‹æ±‚ã€‚é‡‘é‘° (é®ç½©å¾Œ): {'*' * (len(request_data.api_key) - 4) + request_data.api_key[-4:] if len(request_data.api_key) > 4 else '***'}...")
    is_successfully_validated = False
    try:
        genai.configure(api_key=request_data.api_key)
        # å¯¦éš›é©—è­‰ï¼šå˜—è©¦åˆ—å‡ºæ¨¡å‹ï¼Œå¦‚æœå¤±æ•—å‰‡é‡‘é‘°å¯èƒ½ç„¡æ•ˆæˆ–æ¬Šé™ä¸è¶³
        next(genai.list_models(), None) # å˜—è©¦è¿­ä»£ä¸€å€‹ä»¥è§¸ç™¼æ½›åœ¨éŒ¯èª¤
        is_successfully_validated = True
    except Exception as e_val:
        logger.error(f"[ERROR] é©—è­‰æä¾›çš„ API é‡‘é‘°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e_val}")
        is_successfully_validated = False

    if is_successfully_validated:
        global_api_key = request_data.api_key
        api_key_is_valid = True
        logger.info(f"[SUCCESS] è‡¨æ™‚ API é‡‘é‘°å·²è¨­å®šä¸¦é©—è­‰æˆåŠŸã€‚")
        return {"message": "API é‡‘é‘°å·²è¨­å®šä¸¦é©—è­‰æˆåŠŸã€‚"}
    else:
        api_key_is_valid = False # ç¢ºä¿æ¨™è¨˜ç‚ºç„¡æ•ˆ
        logger.error(f"[ERROR] æä¾›çš„è‡¨æ™‚ API é‡‘é‘°é©—è­‰å¤±æ•—ã€‚")
        raise HTTPException(status_code=400, detail="æä¾›çš„ API é‡‘é‘°é©—è­‰å¤±æ•—ã€‚è«‹æª¢æŸ¥é‡‘é‘°æ˜¯å¦æ­£ç¢ºä¸”å…·æœ‰æ‰€éœ€æ¬Šé™ã€‚")

@app.get("/api/check_api_key_status")
async def check_api_key_status():
    global global_api_key, api_key_is_valid
    if global_api_key and api_key_is_valid:
        logger.debug("API é‡‘é‘°ç‹€æ…‹æª¢æŸ¥ï¼šå·²è¨­å®šä¸”æœ‰æ•ˆã€‚")
        return {"status": "set_and_valid", "message": "API é‡‘é‘°å·²è¨­å®šä¸”æœ‰æ•ˆã€‚"}
    elif global_api_key and not api_key_is_valid:
        logger.debug("API é‡‘é‘°ç‹€æ…‹æª¢æŸ¥ï¼šå·²è¨­å®šä½†é©—è­‰å¤±æ•—ã€‚")
        return {"status": "set_but_invalid", "message": "API é‡‘é‘°å·²è¨­å®šä½†é©—è­‰å¤±æ•—ï¼Œè«‹é‡æ–°è¨­å®šã€‚"}
    else:
        logger.debug("API é‡‘é‘°ç‹€æ…‹æª¢æŸ¥ï¼šæœªè¨­å®šã€‚")
        return {"status": "not_set", "message": "API é‡‘é‘°å°šæœªè¨­å®šï¼Œè«‹è¨­å®š API é‡‘é‘°ã€‚"}


# --- è¼”åŠ©å‡½å¼ (sanitize_filename, generate_html_report_content_via_jinja) ---
def sanitize_base_filename(title: str, max_length: int = 50) -> str:
    if not title: title = "untitled_audio"
    title = str(title)
    title = re.sub(r'[\\/:*?"<>|&]', "_", title)
    title = title.replace(" ", "_")
    title = re.sub(r"_+", "_", title)
    title = title.strip('_')
    if title.startswith('.'): title = "_" + title[1:]
    return title[:max_length]

def generate_html_report_content_via_jinja(report_title: str, summary_data: Optional[Dict], transcript_data: Optional[Dict], model_id: str) -> str:
    try:
        return report_content_jinja_template.render(
            report_title=report_title,
            summary_data=summary_data,
            transcript_paragraphs=transcript_data,
            model_id=model_id
        )
    except Exception as e:
        logger.error(f"Jinja2 æ¨¡æ¿æ¸²æŸ“å ±å‘Šå…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        traceback.print_exc()
        return f"<div class='report-content'><p style='color:red;'>æŠ±æ­‰ï¼Œç”Ÿæˆå ±å‘Šé è¦½æ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤ï¼š{str(e)}</p></div>"

# --- process_audio_and_generate_report_task (å¼·åŒ–éŒ¯èª¤è™•ç†) ---
async def process_audio_and_generate_report_task(task_id: str, request_data: GenerateReportRequest):
    tasks_db[task_id]["status"] = "processing"
    tasks_db[task_id]["start_time"] = datetime.now(timezone.utc).isoformat()
    logger.info(f"[TASK {task_id}] è™•ç†é–‹å§‹: {request_data.source_path} (æ¨¡å‹: {request_data.model_id})")

    if not global_api_key or not api_key_is_valid:
        tasks_db[task_id]["status"] = "failed"
        tasks_db[task_id]["error_message"] = "API é‡‘é‘°ç„¡æ•ˆæˆ–æœªè¨­å®šæ–¼èƒŒæ™¯ä»»å‹™å•Ÿå‹•æ™‚ã€‚è«‹å…ˆè¨­å®šæœ‰æ•ˆçš„ API é‡‘é‘°ã€‚"
        tasks_db[task_id]["completion_time"] = datetime.now(timezone.utc).isoformat()
        logger.error(f"[TASK {task_id}] [ERROR] å¤±æ•— - API é‡‘é‘°ç„¡æ•ˆã€‚"); return

    source_basename = os.path.basename(request_data.source_path)
    audio_file_for_api = None # Initialize outside try block for access in finally

    try:
        # Ensure genai is configured with the API key for this task
        genai.configure(api_key=global_api_key)
        logger.info(f"[TASK {task_id}] genai configured with API key for task.")

        # Upload audio file
        logger.info(f"[TASK {task_id}] Starting audio file upload for: {request_data.source_path}")
        try:
            audio_file_for_api = genai.upload_file(path=request_data.source_path)
            logger.info(f"[TASK {task_id}] Successfully uploaded audio file. File ID: {audio_file_for_api.name}, URI: {audio_file_for_api.uri}")
        except Exception as e_upload:
            logger.error(f"[TASK {task_id}] Error uploading audio file: {e_upload}")
            tasks_db[task_id]["status"] = "failed"
            tasks_db[task_id]["error_message"] = f"Audio file upload failed: {e_upload}"
            tasks_db[task_id]["completion_time"] = datetime.now(timezone.utc).isoformat()
            return # Exit if upload fails

        # Instantiate Gemini Model
        model = genai.GenerativeModel(request_data.model_id)
        logger.info(f"[TASK {task_id}] Gemini model '{request_data.model_id}' instantiated.")

        # Prompt Selection
        summary_prompt_to_use = DEFAULT_SUMMARY_PROMPT_PY
        if request_data.custom_prompts and request_data.custom_prompts.get("summary_prompt"):
            summary_prompt_to_use = request_data.custom_prompts["summary_prompt"]
            logger.info(f"[TASK {task_id}] Using custom summary prompt.")
        else:
            logger.info(f"[TASK {task_id}] Using default summary prompt.")

        transcript_prompt_to_use = DEFAULT_TRANSCRIPT_PROMPT_PY
        if request_data.custom_prompts and request_data.custom_prompts.get("transcript_prompt"):
            transcript_prompt_to_use = request_data.custom_prompts["transcript_prompt"]
            logger.info(f"[TASK {task_id}] Using custom transcript prompt.")
        else:
            logger.info(f"[TASK {task_id}] Using default transcript prompt.")

        actual_summary_text: Optional[str] = None
        actual_transcript_text: Optional[str] = None

        # Placeholder for actual summary generation
        if "summary_tc" in request_data.output_options or \
           "summary_transcript_tc" in request_data.output_options or \
           "transcript_bilingual_summary" in request_data.output_options:
            logger.info(f"[TASK {task_id}] Generating summary using model {request_data.model_id} with prompt: '{summary_prompt_to_use[:100]}...'")
            try:
                logger.info(f"[TASK {task_id}] Calling Gemini API for summary with model {request_data.model_id}. Audio URI: {audio_file_for_api.uri}")
                response_summary = model.generate_content(
                    [summary_prompt_to_use, audio_file_for_api],
                    generation_config=genai.types.GenerationConfig(
                        # candidate_count=1, # Ensure only one candidate is generated
                        # temperature=0.7 # Adjust as needed
                    )
                )
                actual_summary_text = response_summary.text
                logger.info(f"[TASK {task_id}] Successfully received summary from API. Text length: {len(actual_summary_text)}")
            except Exception as e_summary:
                logger.error(f"[TASK {task_id}] Error generating summary via Gemini API: {e_summary}")
                actual_summary_text = f"Error generating summary: {e_summary}" # Store error message

        # Actual API Call for Transcript
        if "summary_transcript_tc" in request_data.output_options or \
           "transcript_bilingual_summary" in request_data.output_options:
            logger.info(f"[TASK {task_id}] Generating transcript using model {request_data.model_id} with prompt: '{transcript_prompt_to_use[:100]}...'")
            try:
                logger.info(f"[TASK {task_id}] Calling Gemini API for transcript with model {request_data.model_id}. Audio URI: {audio_file_for_api.uri}")
                response_transcript = model.generate_content(
                    [transcript_prompt_to_use, audio_file_for_api],
                    generation_config=genai.types.GenerationConfig(
                        # candidate_count=1,
                        # temperature=0.7
                    )
                )
                actual_transcript_text = response_transcript.text
                logger.info(f"[TASK {task_id}] Successfully received transcript from API. Text length: {len(actual_transcript_text)}")
            except Exception as e_transcript:
                logger.error(f"[TASK {task_id}] Error generating transcript via Gemini API: {e_transcript}")
                actual_transcript_text = f"Error generating transcript: {e_transcript}" # Store error message

        # è½‰æ›ç‚ºçµæ§‹åŒ–è³‡æ–™
        structured_summary_data = None
        logger.info(f"[TASK {task_id}] Parsing summary text (length: {len(actual_summary_text) if actual_summary_text else 0}).")
        if actual_summary_text and actual_summary_text.startswith("Error generating summary:"):
            structured_summary_data = {"intro_paragraph": actual_summary_text, "items": []}
        elif actual_summary_text:
            lines = [line.strip() for line in actual_summary_text.strip().split('\n') if line.strip()] # Remove empty lines and strip
            intro = ""
            items = []
            current_item_details = []
            current_subtitle = None

            # Try to find the first paragraph as intro more reliably
            # The first non-subtitle line could be the intro.
            # Or, if the first line is not a subtitle, it's the intro.
            if lines and not re.match(r"^\s*\*\*(.*?)\*\*\s*$", lines[0]):
                intro = lines.pop(0)

            for line in lines:
                subtitle_match = re.match(r"^\s*\*\*(.*?)\*\*\s*$", line) # Allow leading/trailing spaces for subtitle
                if subtitle_match:
                    if current_subtitle: # Save previous item
                        items.append({"subtitle": current_subtitle, "details": list(current_item_details)})
                    current_subtitle = subtitle_match.group(1).strip()
                    current_item_details.clear()
                    if not intro and items: # If intro was not captured and we are already into items, means no dedicated intro line
                        pass
                elif current_subtitle: # This line is part of current_subtitle's details
                    detail_text = line
                    if line.startswith("- "):
                        detail_text = line[2:].strip()

                    if detail_text: # Add non-empty details
                        if current_item_details and not line.startswith("- "): # Continuation of previous detail
                            current_item_details[-1] += "\n" + detail_text
                        else:
                            current_item_details.append(detail_text)
                elif not intro: # If no subtitle context yet, and it's not a subtitle line, it could be part of intro
                    intro += ("\n" if intro else "") + line


            if current_subtitle: # Add the last item
                items.append({"subtitle": current_subtitle, "details": list(current_item_details)})

            if not items and intro and not actual_summary_text.startswith("Error"): # If no items were parsed, the whole text might be the intro
                # This check helps if the summary is just a single block of text without specific formatting.
                pass # intro is already set

            bilingual_append_text = None # This logic was specific to simulation, might remove or adapt if bilingual output is different
            # if "transcript_bilingual_summary" in request_data.output_options and lines and "(This is the English part" in lines[-1]:
            #     bilingual_append_text = lines.pop(-1)

            structured_summary_data = {"intro_paragraph": intro.strip(), "items": items, "bilingual_append": bilingual_append_text}
        else: # actual_summary_text is None or empty
            structured_summary_data = {"intro_paragraph": "No summary content received or summary was empty.", "items": []}
        logger.debug(f"[TASK {task_id}] Parsed summary data: intro_len={len(structured_summary_data['intro_paragraph'])}, items_count={len(structured_summary_data['items'])}")

        structured_transcript_data = None
        logger.info(f"[TASK {task_id}] Parsing transcript text (length: {len(actual_transcript_text) if actual_transcript_text else 0}).")
        if actual_transcript_text and actual_transcript_text.startswith("Error generating transcript:"):
            structured_transcript_data = {"paragraphs": [{"content": actual_transcript_text, "is_speaker_line": False, "speaker": None, "insert_hr_after": False}]}
        elif actual_transcript_text:
            paragraphs_raw = actual_transcript_text.strip().split('\n')
            hr_interval = 5 # This can be kept or removed based on desired output styling
            formatted_paragraphs = []
            bilingual_prepend_text = None # This logic was specific to simulation
            # if "transcript_bilingual_summary" in request_data.output_options and paragraphs_raw and paragraphs_raw[0].startswith("(Original Language Transcript"):
            #     bilingual_prepend_text = paragraphs_raw.pop(0)

            for i, p_text in enumerate(paragraphs_raw):
                p_text_stripped = p_text.strip()
                if p_text_stripped: # Process non-empty paragraphs
                    # Regex for speaker: "ç™¼è¨€è€… A:", "Speaker B:", "ç™¼è¨€è€…Cï¼š", etc.
                    # Made it more flexible with speaker names (word characters) and colon/space variations.
                    match = re.match(r"^(ç™¼è¨€è€…\s*[\w\d]+|Speaker\s*[\w\d]+)\s*[:ï¼š]\s*(.*)", p_text_stripped, re.IGNORECASE)
                    formatted_paragraphs.append({
                        "content": match.group(2) if match else p_text_stripped,
                        "is_speaker_line": bool(match),
                        "speaker": match.group(1).strip() if match else None,
                        "insert_hr_after": (i + 1) % hr_interval == 0 and i < len(paragraphs_raw) - 1 and bool(match) # HR only after speaker lines for now
                    })
            structured_transcript_data = {"bilingual_prepend": bilingual_prepend_text, "paragraphs": formatted_paragraphs}
        else: # actual_transcript_text is None or empty
            structured_transcript_data = {"paragraphs": [{"content": "No transcript content received or transcript was empty.", "is_speaker_line": False, "speaker": None, "insert_hr_after": False}]}
        logger.debug(f"[TASK {task_id}] Parsed transcript data: paragraphs_count={len(structured_transcript_data['paragraphs'])}")

        # æª”æ¡ˆç”Ÿæˆé‚è¼¯
        tasks_db[task_id]["status"] = "generating_report"
        # await asyncio.sleep(1) # Original delay, can be removed or adjusted

        report_title = f"'{source_basename}' çš„ AI åˆ†æå ±å‘Š"
        preview_html = generate_html_report_content_via_jinja(report_title, structured_summary_data, structured_transcript_data, request_data.model_id)

        base_report_filename = f"{sanitize_base_filename(source_basename, 30)}_{request_data.model_id}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        download_links = {}

        # ç”Ÿæˆå®Œæ•´çš„ HTML å ±å‘Š
        full_html_content = f"""<!DOCTYPE html><html lang="zh-Hant"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>{report_title}</title><link rel="stylesheet" href="/static/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"></head><body class="dark-mode"><div class="container content-wrapper" style="padding-top: 20px; padding-bottom: 20px;"><section class="card animated" style="margin-bottom:0;">{preview_html}</section></div></body></html>"""
        html_file_path = os.path.join(GENERATED_REPORTS_DIR, f"{base_report_filename}.html")
        try:
            with open(html_file_path, "w", encoding="utf-8") as f:
                f.write(full_html_content)
            download_links["html"] = f"/generated_reports/{os.path.basename(html_file_path)}"
        except Exception as e:
            logger.error(f"[TASK {task_id}] [ERROR] å„²å­˜ HTML å ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        # ç”Ÿæˆ Markdown å ±å‘Š
        if "md" in request_data.output_options:
            md_parts = [f"# {report_title}\n\n"]
            if structured_summary_data:
                md_parts.extend(
                    [f"## é‡é»æ‘˜è¦\n\n{structured_summary_data['intro_paragraph']}\n\n" if structured_summary_data.get('intro_paragraph') else "## é‡é»æ‘˜è¦\n\n"] +
                    [f"### {item['subtitle']}\n" + "".join([f"- {d}\n" for d in item.get('details', [])]) + "\n" for item in structured_summary_data.get('items', [])] +
                    ([f"{structured_summary_data['bilingual_append']}\n\n"] if structured_summary_data.get('bilingual_append') else [])
                )
            if structured_transcript_data:
                md_parts.extend(
                    ["## é€å­—ç¨¿\n\n"] +
                    ([f"{structured_transcript_data['bilingual_prepend']}\n\n"] if structured_transcript_data.get('bilingual_prepend') else []) +
                    [f"**{p['speaker']}:** {p['content']}\n\n" if p["is_speaker_line"] else f"{p['content']}\n\n" for p in structured_transcript_data.get('paragraphs', [])]
                )
            md_file_path = os.path.join(GENERATED_REPORTS_DIR, f"{base_report_filename}.md")
            try:
                with open(md_file_path, "w", encoding="utf-8") as f:
                    f.write("".join(md_parts))
                download_links["md"] = f"/generated_reports/{os.path.basename(md_file_path)}"
            except Exception as e:
                logger.error(f"[TASK {task_id}] [ERROR] å„²å­˜ Markdown å ±å‘ŠéŒ¯èª¤: {e}")

        # ç”Ÿæˆ TXT å ±å‘Š
        if "txt" in request_data.output_options:
            txt_parts = [f"{report_title}\n\n"]
            if structured_summary_data:
                txt_parts.extend(
                    ["é‡é»æ‘˜è¦\n--------------------\n"] +
                    ([f"{structured_summary_data['intro_paragraph']}\n\n"] if structured_summary_data.get('intro_paragraph') else []) +
                    [f"{item['subtitle']}\n" + "".join([f"  - {d}\n" for d in item.get('details', [])]) + "\n" for item in structured_summary_data.get('items', [])] +
                    ([f"{structured_summary_data['bilingual_append']}\n\n"] if structured_summary_data.get('bilingual_append') else [])
                )
            if structured_transcript_data:
                txt_parts.extend(
                    ["é€å­—ç¨¿\n--------------------\n"] +
                    ([f"{structured_transcript_data['bilingual_prepend']}\n\n"] if structured_transcript_data.get('bilingual_prepend') else []) +
                    [f"{p['speaker']}: {p['content']}\n\n" if p["is_speaker_line"] else f"{p['content']}\n\n" for p in structured_transcript_data.get('paragraphs', [])]
                )
            txt_file_path = os.path.join(GENERATED_REPORTS_DIR, f"{base_report_filename}.txt")
            try:
                with open(txt_file_path, "w", encoding="utf-8") as f:
                    f.write("".join(txt_parts))
                download_links["txt"] = f"/generated_reports/{os.path.basename(txt_file_path)}"
            except Exception as e:
                logger.error(f"[TASK {task_id}] [ERROR] å„²å­˜ TXT å ±å‘ŠéŒ¯èª¤: {e}")

        tasks_db[task_id]["status"] = "completed"
        tasks_db[task_id]["result_preview_html"] = preview_html
        tasks_db[task_id]["download_links"] = download_links
        tasks_db[task_id]["completion_time"] = datetime.now(timezone.utc).isoformat()
        logger.info(f"[TASK {task_id}] [SUCCESS] è™•ç†å®Œæˆã€‚")

    except Exception as e:
        tasks_db[task_id]["status"] = "failed"
        tasks_db[task_id]["error_message"] = f"èƒŒæ™¯ä»»å‹™è™•ç†å¤±æ•—: {str(e)}"
        tasks_db[task_id]["completion_time"] = datetime.now(timezone.utc).isoformat()
        logger.error(f"[TASK {task_id}] [ERROR] èƒŒæ™¯ä»»å‹™è™•ç†å¤±æ•—: {e}")
        traceback.print_exc()
    finally:
        if audio_file_for_api:
            try:
                logger.info(f"[TASK {task_id}] Deleting uploaded file {audio_file_for_api.uri} ({audio_file_for_api.name}) from Google Cloud.")
                genai.delete_file(audio_file_for_api.name) # Use .name for deletion
                logger.info(f"[TASK {task_id}] Successfully deleted file {audio_file_for_api.uri}.")
            except Exception as e_delete_file:
                logger.warning(f"[TASK {task_id}] Failed to delete uploaded file {audio_file_for_api.uri}: {e_delete_file}")


# --- éŸ³è¨Šä¾†æºè™•ç† API ---
def _download_youtube_audio_sync(youtube_url: str, task_id_for_log: Optional[str]="N/A") -> str:
    logger.info(f"[TASK {task_id_for_log}] [SYNC_DOWNLOAD] é–‹å§‹ä¸‹è¼‰ YouTube éŸ³è¨Š: {youtube_url}")
    try:
        yt = YouTube(youtube_url)
        audio_stream = yt.streams.get_audio_only()
        if not audio_stream:
            audio_stream = yt.streams.filter(only_audio=True, file_extension='m4a').order_by('abr').desc().first()
        if not audio_stream:
            audio_stream = yt.streams.filter(only_audio=True, file_extension='webm').order_by('abr').desc().first()
        if not audio_stream:
            raise PytubeFixError(f"åœ¨ YouTube å½±ç‰‡ '{youtube_url}' ä¸­æ‰¾ä¸åˆ°åˆé©çš„éŸ³è¨Šæµã€‚")

        title_part = sanitize_base_filename(yt.title, max_length=30)
        timestamp_part = datetime.now().strftime("%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        file_extension = audio_stream.subtype if audio_stream.subtype else "mp4"
        final_filename = f"{title_part}_{timestamp_part}_{unique_id}.{file_extension}"
        actual_downloaded_path = audio_stream.download(output_path=TEMP_AUDIO_STORAGE_DIR, filename=final_filename)

        if not os.path.exists(actual_downloaded_path) or os.path.getsize(actual_downloaded_path) == 0:
            raise PytubeFixError(f"YouTube éŸ³è¨Šæª”æ¡ˆ '{final_filename}' ä¸‹è¼‰å¾Œæœªæ‰¾åˆ°æˆ–ç‚ºç©ºã€‚")

        logger.info(f"[TASK {task_id_for_log}] [SYNC_DOWNLOAD] [SUCCESS] YouTube éŸ³è¨ŠæˆåŠŸä¸‹è¼‰è‡³: {actual_downloaded_path}")
        return actual_downloaded_path
    except PytubeFixError as pte:
        error_msg = f"PytubeFix åœ¨è™•ç† YouTube éŸ³è¨Š '{youtube_url}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(pte)}"
        logger.error(f"[TASK {task_id_for_log}] [SYNC_DOWNLOAD] [ERROR] {error_msg}")
        traceback.print_exc()
        raise PytubeFixError(error_msg) from pte
    except Exception as e:
        error_msg = f"ä¸‹è¼‰ YouTube éŸ³è¨Š '{youtube_url}' æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {str(e)}"
        logger.error(f"[TASK {task_id_for_log}] [SYNC_DOWNLOAD] [ERROR] {error_msg}")
        traceback.print_exc()
        raise RuntimeError(error_msg) from e

@app.post("/api/process_youtube_url", response_model=Dict[str, str])
async def api_process_youtube_url(request_data: ProcessUrlRequest):
    log_task_id = str(uuid.uuid4())[:8]
    logger.info(f"[API_YT_URL] [TASK {log_task_id}] æ”¶åˆ° YouTube ç¶²å€è™•ç†è«‹æ±‚: {request_data.url}")
    # Pydantic å·²ç¶“åšäº† URL æ ¼å¼é©—è­‰ï¼Œé€™è£¡å¯ä»¥ç°¡åŒ–
    try:
        loop = asyncio.get_event_loop()
        local_audio_path = await loop.run_in_executor(executor, _download_youtube_audio_sync, request_data.url, log_task_id)
        return {"message": f"YouTube éŸ³è¨Š '{os.path.basename(local_audio_path)}' å·²æˆåŠŸä¸‹è¼‰è‡³ä¼ºæœå™¨ã€‚", "youtube_url": request_data.url, "processed_audio_path": local_audio_path}
    except PytubeFixError as pte:
        raise HTTPException(status_code=500, detail=str(pte))
    except Exception as e:
        logger.error(f"[API_YT_URL] [TASK {log_task_id}] [ERROR] è™•ç† YouTube ç¶²å€æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è™•ç† YouTube ç¶²å€æ™‚ç™¼ç”Ÿä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {str(e)}")

@app.post("/api/upload_audio_file", response_model=Dict[str, str])
async def api_upload_audio_file(audio_file: UploadFile = File(...)):
    log_task_id = str(uuid.uuid4())[:8]
    logger.info(f"[API_UPLOAD] [TASK {log_task_id}] æ”¶åˆ°æª”æ¡ˆä¸Šå‚³: {audio_file.filename}, é¡å‹: {audio_file.content_type}")
    original_name, original_ext = os.path.splitext(audio_file.filename if audio_file.filename else "uploaded_audio")

    # å˜—è©¦å¾ content_type æ¨æ–·å‰¯æª”å
    if not original_ext and audio_file.content_type:
        content_type_map = {
            "audio/mpeg": ".mp3", "audio/mp4": ".m4a", "audio/webm": ".webm",
            "audio/wav": ".wav", "audio/ogg": ".ogg", "audio/aac": ".aac"
        }
        original_ext = content_type_map.get(audio_file.content_type, ".bin") # é è¨­ç‚º .bin ä»¥é˜²è¬ä¸€

    base_name = sanitize_base_filename(original_name, max_length=30)
    timestamp = datetime.now().strftime("%m%d_%H%M%S")
    unique_id = str(uuid.uuid4())[:8]
    temp_upload_filename = f"{base_name}_{timestamp}_{unique_id}{original_ext}"
    temp_upload_path = os.path.join(TEMP_AUDIO_STORAGE_DIR, temp_upload_filename)

    try:
        # ä½¿ç”¨ asyncio.to_thread ç¢ºä¿æª”æ¡ˆå¯«å…¥ä¸æœƒé˜»å¡ FastAPI äº‹ä»¶å¾ªç’°
        await asyncio.to_thread(shutil.copyfileobj, audio_file.file, open(temp_upload_path, "wb"))
        logger.info(f"[API_UPLOAD] [TASK {log_task_id}] [SUCCESS] æª”æ¡ˆæˆåŠŸå„²å­˜è‡³: {temp_upload_path} (å¤§å°: {os.path.getsize(temp_upload_path)} bytes)")
        return {"message": f"éŸ³è¨Šæª”æ¡ˆ '{audio_file.filename}' ä¸Šå‚³ä¸¦å„²å­˜æˆåŠŸã€‚", "filename": audio_file.filename, "content_type": audio_file.content_type, "processed_audio_path": temp_upload_path}
    except Exception as e:
        logger.error(f"[API_UPLOAD] [TASK {log_task_id}] [ERROR] å„²å­˜ä¸Šå‚³æª”æ¡ˆå¤±æ•—: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è™•ç†ä¸Šå‚³æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    finally:
        await audio_file.close()


# --- AI æ¨¡å‹èˆ‡å ±å‘Šç”Ÿæˆ API ---

# é å®šç¾©æ¨¡å‹è³‡æ–™ (åŒ…å«ä¸­æ–‡ä»‹ç´¹å’Œæ’åºè³‡è¨Š)
PREDEFINED_MODELS_DATA_APP = {
    "models/gemini-1.5-flash-latest": {
        "id": "models/gemini-1.5-flash-latest",
        "dropdown_display_name": "âš¡ Gemini 1.5 Flash (æœ€æ–°ç‰ˆ)",
        "chinese_display_name": "Gemini 1.5 Flash æœ€æ–°ç‰ˆ",
        "chinese_summary_parenthesized": "ï¼ˆé€Ÿåº¦å¿«ã€å¤šåŠŸèƒ½ã€å¤šæ¨¡æ…‹ã€é©ç”¨æ–¼å¤šæ¨£åŒ–ä»»å‹™æ“´å±•ï¼‰",
        "chinese_input_output": "è¼¸å…¥ï¼šæ–‡å­—ã€éŸ³è¨Šã€åœ–ç‰‡ã€å½±ç‰‡ (è©³è¦‹å®˜æ–¹æ–‡ä»¶)ï¼›è¼¸å‡ºï¼šæ–‡å­—",
        "chinese_suitable_for": "éœ€è¦å¿«é€Ÿå›æ‡‰çš„å¤šæ¨£åŒ–ä»»å‹™ã€å¤§è¦æ¨¡æ‡‰ç”¨ã€èŠå¤©ã€æ‘˜è¦ã€éŸ³è¨Šè™•ç†ã€‚",
        "original_description_from_api": "Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.",
        "sort_priority": 1 # æ’åºå„ªå…ˆç´šï¼Œæ•¸å­—è¶Šå°è¶Šé å‰
    },
    "models/gemini-1.5-pro-latest": {
        "id": "models/gemini-1.5-pro-latest",
        "dropdown_display_name": "ğŸŒŸ Gemini 1.5 Pro (æœ€æ–°ç‰ˆ)",
        "chinese_display_name": "Gemini 1.5 Pro æœ€æ–°ç‰ˆ",
        "chinese_summary_parenthesized": "ï¼ˆåŠŸèƒ½å¼·å¤§ã€å¤§å‹ä¸Šä¸‹æ–‡ã€å¤šæ¨¡æ…‹ã€ç†è§£è¤‡é›œæƒ…å¢ƒï¼‰",
        "chinese_input_output": "è¼¸å…¥ï¼šæ–‡å­—ã€éŸ³è¨Šã€åœ–ç‰‡ã€å½±ç‰‡ (æœ€é«˜é”2ç™¾è¬tokenï¼Œè©³è¦‹å®˜æ–¹æ–‡ä»¶)ï¼›è¼¸å‡ºï¼šæ–‡å­—",
        "chinese_suitable_for": "è¤‡é›œæ¨ç†ã€é•·ç¯‡å…§å®¹ç†è§£èˆ‡ç”Ÿæˆã€å¤šæ¨¡æ…‹åˆ†æã€ç¨‹å¼ç¢¼ç”Ÿæˆèˆ‡è§£é‡‹ã€‚",
        "original_description_from_api": "Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.",
        "sort_priority": 0 # æœ€é«˜å„ªå…ˆç´š
    },
    "models/gemini-pro": { # å‡è¨­é€™æ˜¯ç´”æ–‡å­—çš„ gemini-pro
        "id": "models/gemini-pro",
        "dropdown_display_name": "Gemini Pro (ç´”æ–‡å­—)",
        "chinese_display_name": "Gemini Pro (ç´”æ–‡å­—ç‰ˆ)",
        "chinese_summary_parenthesized": "ï¼ˆå„ªåŒ–çš„ç´”æ–‡å­—ç”Ÿæˆèˆ‡ç†è§£ï¼‰",
        "chinese_input_output": "è¼¸å…¥ï¼šæ–‡å­—ï¼›è¼¸å‡ºï¼šæ–‡å­—",
        "chinese_suitable_for": "ç´”æ–‡å­—çš„å•ç­”ã€æ‘˜è¦ã€å¯«ä½œã€ç¿»è­¯ç­‰ã€‚",
        "original_description_from_api": "Optimized for text-only prompts.",
        "sort_priority": 10
    }
    # æ‚¨å¯ä»¥æ ¹æ“šéœ€è¦åŠ å…¥æ›´å¤šé å®šç¾©æ¨¡å‹å’Œå®ƒå€‘çš„ä¸­æ–‡è³‡è¨Š
}

def get_model_version_score(api_name_lower: str) -> int:
    score = 9999 # é è¨­è¼ƒä½å„ªå…ˆç´š
    if "latest" in api_name_lower: score = 0
    elif "preview" in api_name_lower:
        score = 1000
        date_match = re.search(r'preview[_-](\d{2})[_-]?(\d{2})', api_name_lower) # ä¾‹å¦‚ preview-0527
        if date_match:
            try:
                # çµ„åˆæ—¥æœŸï¼Œä¾‹å¦‚ 0527 -> 527ï¼Œæ•¸å­—è¶Šå¤§ä»£è¡¨æ—¥æœŸè¶Šæ–°
                date_score = int(date_match.group(1)) * 100 + int(date_match.group(2))
                score -= date_score # æ•¸å­—è¶Šå¤§è¶Šæ–°ï¼Œæ‰€ä»¥è¦æ¸›ï¼Œè®“ç¸½åˆ†æ•¸è®Šå°
            except ValueError:
                pass # è§£æå¤±æ•—å‰‡ä½¿ç”¨é è¨­å€¼
        else: score += 100 # ç„¡æ—¥æœŸçš„ preview æ¯”æœ‰æ—¥æœŸçš„èˆŠ
    elif "-exp" in api_name_lower or "experimental" in api_name_lower: score = 2000
    else: # å˜—è©¦è§£ææ•¸å­—ç‰ˆæœ¬è™Ÿï¼Œä¾‹å¦‚ -001
        num_version_match = re.search(r'-(\d+)$', api_name_lower.split('/')[-1])
        if num_version_match:
            try:
                score = 3000 - int(num_version_match.group(1)) # æ•¸å­—è¶Šå¤§ç‰ˆæœ¬è¶Šæ–°ï¼Œæ‰€ä»¥æ¸›
            except ValueError:
                score = 3500
    return score

def sort_models_key_function(model_dict: Dict[str, Any]):
    api_name = model_dict.get("id", "")
    name_lower = api_name.lower()

    # å„ªå…ˆç´šçµ„ (ä¾†è‡ªé å®šç¾©è³‡æ–™æˆ–æ ¹æ“šåç¨±çŒœæ¸¬)
    priority_group = model_dict.get("sort_priority", 99) # é å®šç¾©çš„å„ªå…ˆç´š
    if priority_group == 99: # å¦‚æœæ²’æœ‰é å®šç¾©çš„ï¼Œå˜—è©¦çŒœæ¸¬
        if "gemini-1.5-pro" in name_lower: priority_group = 2 # æ¯” flash latest ç¨ä½
        elif "gemini-1.5-flash" in name_lower: priority_group = 3
        elif "gemini-pro" in name_lower and "vision" not in name_lower : priority_group = 12
        elif "gemini" in name_lower: priority_group = 15
        else: priority_group = 20

    version_score = get_model_version_score(name_lower)

    # ä¸»è¦ç‰ˆæœ¬è™Ÿ (ä¾‹å¦‚ 1.5, 1.0) ç”¨æ–¼æ¬¡ç´šæ’åº
    main_version_num = 0.0
    main_version_match = re.search(r'(gemini(?:-1\.5)?)-(\d+\.\d+|\d+)', name_lower) # gemini-1.5-pro, gemini-pro
    if not main_version_match: main_version_match = re.search(r'gemini-(\d+\.\d+|\d+)', name_lower)

    if main_version_match:
        try:
            main_version_num = float(main_version_match.groups()[-1]) # å–æœ€å¾Œæ•ç²çš„æ•¸å­—çµ„
        except ValueError:
            pass

    # æ’åºè¦å‰‡ï¼šå„ªå…ˆç´šçµ„ (è¶Šå°è¶Šå„ªå…ˆ) -> ä¸»è¦ç‰ˆæœ¬è™Ÿ (è¶Šå¤§è¶Šå„ªå…ˆï¼Œæ‰€ä»¥å–è² å€¼) -> ç‰ˆæœ¬åˆ†æ•¸ (è¶Šå°è¶Šå„ªå…ˆ) -> å­—æ¯é †åº
    return (priority_group, -main_version_num, version_score, name_lower)


@app.get("/api/get_models")
async def api_get_models_enhanced():
    logger.info("[API_GET_MODELS_ENHANCED] è«‹æ±‚ç²å–å¢å¼·å‹ AI æ¨¡å‹åˆ—è¡¨ã€‚")

    if not api_key_is_valid: # API Key ç„¡æ•ˆæ™‚ä¸å˜—è©¦åˆ—å‡ºæ¨¡å‹
        logger.warning("[API_GET_MODELS_ENHANCED] API Key ç„¡æ•ˆï¼Œè¿”å›åŒ…å«éŒ¯èª¤è¨Šæ¯çš„æ¨¡å‹åˆ—è¡¨ã€‚")
        return JSONResponse(content=[
            {"id": "error-api-key-or-network",
             "dropdown_display_name": "éŒ¯èª¤ï¼šç„¡æ³•ç²å–æ¨¡å‹ (APIé‡‘é‘°ç„¡æ•ˆ)",
             "chinese_display_name": "APIé‡‘é‘°ç„¡æ•ˆæˆ–ç¶²è·¯å•é¡Œ",
             "chinese_summary_parenthesized": "ï¼ˆè«‹æª¢æŸ¥æ‚¨çš„ Google API é‡‘é‘°æ˜¯å¦æ­£ç¢ºä¸”æœ‰æ¬Šé™è¨ªå•æ¨¡å‹ï¼‰",
             "chinese_input_output": "N/A",
             "chinese_suitable_for": "è«‹åœ¨å·¦å´è¨­å®šå€è¼¸å…¥æœ‰æ•ˆçš„ API é‡‘é‘°ã€‚",
             "original_description_from_api": "API Key is invalid or not set. Failed to retrieve models from Google API.",
             "sort_priority": -1 # æœ€é«˜å„ªå…ˆç´šï¼Œç¢ºä¿åœ¨åˆ—è¡¨é ‚éƒ¨
            }
        ], status_code=500) # è¿”å› 500 éŒ¯èª¤ç‹€æ…‹ï¼Œè¡¨ç¤ºä¼ºæœå™¨ç«¯ç²å–æ¨¡å‹å¤±æ•—

    all_models_combined = {}
    try:
        logger.debug("[API_GET_MODELS_ENHANCED] æ­£åœ¨å¾ Google genai.list_models() æŸ¥è©¢ç·šä¸Šæ¨¡å‹...")
        online_models_count = 0
        for m_obj in genai.list_models():
            # ç¢ºä¿æ¨¡å‹æ”¯æ´ generateContent æ–¹æ³•
            if 'generateContent' in m_obj.supported_generation_methods:
                online_models_count +=1
                model_data = {
                    "id": m_obj.name,
                    "dropdown_display_name": m_obj.display_name if m_obj.display_name else m_obj.name.replace("models/", ""),
                    "chinese_display_name": m_obj.display_name if m_obj.display_name else m_obj.name.replace("models/", ""), # é è¨­ä½¿ç”¨ display_name
                    "chinese_summary_parenthesized": "", # é è¨­ç©º
                    "chinese_input_output": f"è¼¸å…¥ Tokens: {m_obj.input_token_limit}, è¼¸å‡º Tokens: {m_obj.output_token_limit}",
                    "chinese_suitable_for": "è«‹åƒè€ƒ API åŸå§‹æè¿°ã€‚",
                    "original_description_from_api": m_obj.description if m_obj.description else "N/A",
                    "sort_priority": 99 # é è¨­ä¸€å€‹è¼ƒä½çš„å„ªå…ˆç´šçµ¦ç·šä¸Šç²å–çš„ã€æœªåœ¨é å®šç¾©ä¸­å‡ºç¾çš„æ¨¡å‹
                }
                all_models_combined[m_obj.name] = model_data
        logger.debug(f"[API_GET_MODELS_ENHANCED] å¾ API ç²å–åˆ° {online_models_count} å€‹æ”¯æ´ generateContent çš„æ¨¡å‹ã€‚")

    except Exception as e_list_models:
        logger.error(f"[API_GET_MODELS_ENHANCED] å‘¼å« genai.list_models() å¤±æ•—: {e_list_models}")
        # å¦‚æœ API å‘¼å«å¤±æ•—ï¼Œè¿”å›æ˜ç¢ºçš„éŒ¯èª¤æ¨¡å‹ç‹€æ…‹çµ¦å‰ç«¯
        return JSONResponse(content=[
            {"id": "error-api-key-or-network",
             "dropdown_display_name": "éŒ¯èª¤ï¼šç„¡æ³•ç²å–æ¨¡å‹ (APIé‡‘é‘°æˆ–ç¶²è·¯å•é¡Œ)",
             "chinese_display_name": "APIé‡‘é‘°æˆ–ç¶²è·¯å•é¡Œ",
             "chinese_summary_parenthesized": "ï¼ˆç„¡æ³•é€£ç·šåˆ° Google APIï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ–é‡‘é‘°ç‹€æ…‹ï¼‰",
             "chinese_input_output": "N/A",
             "chinese_suitable_for": "è«‹æª¢æŸ¥æ‚¨çš„ API é‡‘é‘°æ˜¯å¦æ­£ç¢ºï¼Œä¸¦ç¢ºä¿ç¶²è·¯é€£æ¥æ­£å¸¸ã€‚",
             "original_description_from_api": "Failed to retrieve models from Google API. Check API key and network connectivity.",
             "sort_priority": -1 # æœ€é«˜å„ªå…ˆç´š
            }
        ], status_code=500) # è¿”å› 500 éŒ¯èª¤ç‹€æ…‹

    # å°‡é å®šç¾©çš„è³‡æ–™è¦†è“‹æˆ–æ·»åŠ åˆ°åˆä½µåˆ—è¡¨ä¸­ (é å®šç¾©çš„å„ªå…ˆ)
    for predefined_id, predefined_data in PREDEFINED_MODELS_DATA_APP.items():
        all_models_combined[predefined_id] = predefined_data # é å®šç¾©çš„è³‡æ–™æœ‰æ›´é«˜å„ªå…ˆç´š

    # è½‰æ›ç‚ºåˆ—è¡¨ä¸¦æ’åº
    sorted_models_list = sorted(all_models_combined.values(), key=sort_models_key_function)

    logger.info(f"[API_GET_MODELS_ENHANCED] è¿”å› {len(sorted_models_list)} å€‹æ¨¡å‹çµ¦å‰ç«¯ã€‚")
    return JSONResponse(content=sorted_models_list)


@app.post("/api/generate_report", status_code=202)
async def api_submit_generate_report_task(request_data: GenerateReportRequest, background_tasks: BackgroundTasks):
    task_id = str(uuid.uuid4())
    logger.info(f"[API_GEN_REPORT] [TASK {task_id}] æ”¶åˆ°å ±å‘Šç”Ÿæˆè«‹æ±‚: ä¾†æº='{request_data.source_path}', æ¨¡å‹='{request_data.model_id}'")

    if not global_api_key or not api_key_is_valid:
        logger.warning(f"[API_GEN_REPORT] [TASK {task_id}] API é‡‘é‘°ç„¡æ•ˆæˆ–æœªè¨­å®šã€‚")
        raise HTTPException(status_code=401, detail="ç„¡æ•ˆæˆ–æœªè¨­å®šçš„ API é‡‘é‘°ã€‚è«‹å…ˆè¨­å®šæœ‰æ•ˆçš„ API é‡‘é‘°ã€‚")

    if not os.path.exists(request_data.source_path):
        logger.error(f"[API_GEN_REPORT] [TASK {task_id}] éŸ³è¨Šä¾†æºæª”æ¡ˆä¸å­˜åœ¨: {request_data.source_path}")
        raise HTTPException(status_code=404, detail=f"æŒ‡å®šçš„éŸ³è¨Šä¾†æºæª”æ¡ˆä¸å­˜åœ¨: {os.path.basename(request_data.source_path)}")

    tasks_db[task_id] = {
        "task_id": task_id,
        "status": "queued",
        "source_name": os.path.basename(request_data.source_path),
        "model_id": request_data.model_id,
        "submit_time": datetime.now(timezone.utc).isoformat(),
        "start_time": None,
        "completion_time": None,
        "result_preview_html": None,
        "download_links": None,
        "error_message": None,
        "request_data": request_data.model_dump()
    }
    background_tasks.add_task(process_audio_and_generate_report_task, task_id, request_data)
    logger.info(f"[API_GEN_REPORT] [TASK {task_id}] ä»»å‹™å·²åŠ å…¥ä½‡åˆ—ã€‚")
    return {"task_id": task_id, "message": "å ±å‘Šç”Ÿæˆä»»å‹™å·²åŠ å…¥ä½‡åˆ—ã€‚", "status": "queued"}


# --- ä»»å‹™ç‹€æ…‹æŸ¥è©¢ API ---
@app.get("/api/tasks")
async def get_all_tasks_status():
    summary_tasks = [{
        "task_id": td["task_id"],
        "status": td["status"],
        "source_name": td["source_name"],
        "model_id": td["model_id"],
        "submit_time": td["submit_time"],
        "error_message": td.get("error_message"),
        "start_time": td.get("start_time"),
        "completion_time": td.get("completion_time"),
        "download_links": td.get("download_links")
    } for td in tasks_db.values()]
    return JSONResponse(content=sorted(summary_tasks, key=lambda x: x["submit_time"], reverse=True))

@app.get("/api/tasks/{task_id}")
async def get_task_status_and_result(task_id: str):
    logger.debug(f"[API_TASK_ID] è«‹æ±‚ç²å–ä»»å‹™ {task_id} çš„è©³ç´°ç‹€æ…‹ã€‚")
    task = tasks_db.get(task_id)
    if not task:
        logger.warning(f"[API_TASK_ID] æ‰¾ä¸åˆ°ä»»å‹™ ID: {task_id}")
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æŒ‡å®šçš„ä»»å‹™ IDã€‚")
    response_data = {
        "task_id": task["task_id"],
        "status": task["status"],
        "source_name": task["source_name"],
        "model_id": task["model_id"],
        "submit_time": task["submit_time"],
        "start_time": task.get("start_time"),
        "completion_time": task.get("completion_time"),
        "error_message": task.get("error_message")
    }
    if task["status"] == "completed":
        response_data["result_preview_html"] = task.get("result_preview_html")
        response_data["download_links"] = task.get("download_links")
    return JSONResponse(content=response_data)


# --- ä¸‹è¼‰ç”Ÿæˆçš„å ±å‘Šæª”æ¡ˆ API ---
try:
    if os.path.exists(GENERATED_REPORTS_DIR):
        app.mount("/generated_reports", StaticFiles(directory=GENERATED_REPORTS_DIR), name="generated_reports")
        logger.info(f"å·²æ›è¼‰ '{GENERATED_REPORTS_DIR}' è‡³ '/generated_reports' ä»¥ä¾›ç›´æ¥ä¸‹è¼‰ã€‚")
except Exception as e:
    logger.error(f"æ›è¼‰ generated_reports ç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# --- ä¸»é é¢ ---
@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    logger.debug("[API_ROOT] è«‹æ±‚ä¸»é é¢ã€‚")
    if templates is None:
        error_msg = "<html><body><h1>éŒ¯èª¤ï¼šä¸»æ¨¡æ¿å¼•æ“æœªåˆå§‹åŒ–ã€‚è«‹æª¢æŸ¥ä¼ºæœå™¨æ—¥èªŒã€‚</h1></body></html>"
        logger.critical("ä¸»æ¨¡æ¿å¼•æ“æœªåˆå§‹åŒ–ã€‚")
        return HTMLResponse(error_msg, status_code=500)
    return templates.TemplateResponse("index.html", {"request": request, "title": "AI_paper æ™ºèƒ½åŠ©ç† v2.4 (ç©©å®šç‰ˆ)"})


# --- API ç‹€æ…‹ ---
@app.get("/api/status")
async def get_api_status():
    logger.debug("[API_STATUS] è«‹æ±‚ API ç‹€æ…‹ã€‚")
    return {"status": "AI_paper API v2.4 is running", "version": "2.4.0_optimized"}

if __name__ == "__main__":
    logger.info("è‹¥åœ¨æœ¬åœ°åŸ·è¡Œ app.pyï¼Œè«‹ç¢ºä¿ CWD è¨­å®šæ­£ç¢ºã€‚")
    uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True, workers=1)



# -*- coding: utf-8 -*-
from fastapi import FastAPI, Request, HTTPException, File, UploadFile, Form, Depends, BackgroundTasks, status
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.exceptions import RequestValidationError
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any
import uvicorn
import os
import shutil
import re
from datetime import datetime, timezone
import traceback
import asyncio
import uuid
from concurrent.futures import ThreadPoolExecutor
import json
import logging # å¼•å…¥ logging æ¨¡çµ„
import sys # ç”¨æ–¼æ›´åš´æ ¼çš„å•Ÿå‹•éŒ¯èª¤è™•ç†
import sqlite3

from pytubefix import YouTube
from pytubefix.exceptions import RegexMatchError, VideoUnavailable, PytubeFixError

import google.generativeai as genai

# --- é…ç½®æ—¥èªŒ (é‡è¦) ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- è¨­å®šå¸¸æ•¸å’Œç›®éŒ„ ---
TEMP_AUDIO_STORAGE_DIR = os.getenv("APP_TEMP_AUDIO_STORAGE_DIR", "./temp_audio")
GENERATED_REPORTS_DIR = os.getenv("APP_GENERATED_REPORTS_DIR", "./generated_reports")
DATABASE_URL = "data/tasks.db" # SQLite è³‡æ–™åº«æª”æ¡ˆè·¯å¾‘
MAX_CONCURRENT_TASKS = 2 # æœ€å¤§ä¸¦è¡Œä»»å‹™æ•¸

# global_api_key: Optional[str] = None # Replaced by dependency injection
# api_key_is_valid: bool = False # Replaced by dependency injection logic
temporary_api_key_storage: Optional[str] = None # Stores API key set via /api/set_api_key

# tasks_db: Dict[str, Dict[str, Any]] = {} # Replaced by SQLite
executor = ThreadPoolExecutor(max_workers=MAX_CONCURRENT_TASKS)

app = FastAPI(title="AI_paper API v2.4 (ç©©å®šæ€§å„ªåŒ–ç‰ˆ - SQLite & DI)")

# --- è‡ªè¨‚ RequestValidationError ä¾‹å¤–è™•ç† (å¼·åŒ–éŒ¯èª¤æ—¥èªŒ) ---
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    error_details = exc.errors()
    logger.error(f"--- [ERROR_VALIDATION] Request to '{request.url}' failed Pydantic validation ---")
    logger.error(f"[ERROR_VALIDATION] Details: {json.dumps(error_details, indent=2, ensure_ascii=False)}")
    logger.error(f"--- [ERROR_VALIDATION] End of Pydantic validation error details ---")

    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content={"detail": error_details},
    )

# --- Jinja2 æ¨¡æ¿è¨­å®š ---
templates_main_dir = "templates"
report_content_template_str = """
<div id='report-title-display' class='report-main-title'>{{ report_title }} (ç”± {{ model_id }} ç”Ÿæˆ)</div>
{% if summary_data %}
<div id='report-summary' class='report-content'>
  <h3><i class='fas fa-lightbulb icon-accent'></i> é‡é»æ‘˜è¦</h3>
  {% if summary_data.intro_paragraph %}
    <p class='intro-paragraph'>{{ summary_data.intro_paragraph }}</p>
  {% endif %}
  {% if summary_data.items %}
    {% for item in summary_data.items %}
      <h3><strong>{{ loop.index }}. {{ item.subtitle }}</strong></h3>
      {% if item.details %}
        <ul>
          {% for detail in item.details %}
            <li>{{ detail }}</li>
          {% endfor %}
        </ul>
      {% endif %}
    {% endfor %}
  {% endif %}
  {% if summary_data.bilingual_append %}
    <p>{{ summary_data.bilingual_append }}</p>
  {% endif %}
</div>
{% endif %}

{% if transcript_paragraphs %}
<div id='report-transcript' class='report-content'>
  <h3><i class='fas fa-file-alt icon-accent'></i> é€å­—ç¨¿</h3>
  {% if transcript_paragraphs.bilingual_prepend %}
    <p>{{ transcript_paragraphs.bilingual_prepend }}</p>
  {% endif %}
  {% for p_item in transcript_paragraphs.paragraphs %}
    {% if p_item.is_speaker_line %}
      <p><strong>{{ p_item.speaker }}:</strong> {{ p_item.content }}</p>
    {% else %}
      <p>{{ p_item.content }}</p>
    {% endif %}
    {% if p_item.insert_hr_after %}
      <hr class='transcript-divider'>
    {% endif %}
  {% endfor %}
</div>
{% endif %}
"""
from jinja2 import Template
report_content_jinja_template = Template(report_content_template_str)

# --- Pydantic æ¨¡å‹å®šç¾© (æ·»åŠ äº†æ›´è©³ç´°çš„ Field é©—è­‰) ---
class ProcessUrlRequest(BaseModel):
    url: str = Field(..., pattern=r"^https?:\/\/(www\.)?(youtube\.com|youtu\.be)\/.*$", description="æœ‰æ•ˆçš„ YouTube ç¶²å€")

class GenerateReportRequest(BaseModel):
    source_type: str = Field(..., pattern="^(youtube|upload)$", description="ä¾†æºé¡å‹ï¼Œåªèƒ½æ˜¯ 'youtube' æˆ– 'upload'")
    source_path: str = Field(..., min_length=1, description="éŸ³è¨Šä¾†æºçš„æœ¬åœ°æª”æ¡ˆè·¯å¾‘")
    model_id: str = Field(..., min_length=1, description="ç”¨æ–¼ç”Ÿæˆå ±å‘Šçš„ AI æ¨¡å‹ ID")
    output_options: List[str] = Field(..., min_items=1, description="å ±å‘Šè¼¸å‡ºæ ¼å¼é¸é …ï¼Œä¾‹å¦‚ 'summary_tc', 'md', 'txt'")
    # custom_prompts ä»ç‚º Optionalï¼Œå‰ç«¯æœƒæ ¹æ“šé‚è¼¯åˆ¤æ–·æ˜¯å¦å‚³é
    custom_prompts: Optional[Dict[str, str]] = Field(None, description="è‡ªè¨‚æç¤ºè©ï¼Œéµç‚º 'summary_prompt' æˆ– 'transcript_prompt'")

class SetApiKeyRequest(BaseModel):
    api_key: str = Field(..., min_length=10, description="Google API é‡‘é‘°")

# --- FastAPI äº‹ä»¶è™•ç† ---
@app.on_event("startup")
async def startup_event():
    os.makedirs(TEMP_AUDIO_STORAGE_DIR, exist_ok=True)
    os.makedirs(GENERATED_REPORTS_DIR, exist_ok=True)
    logger.info(f"è‡¨æ™‚éŸ³è¨Šå„²å­˜ç›®éŒ„ '{TEMP_AUDIO_STORAGE_DIR}' å·²ç¢ºèªå­˜åœ¨ã€‚")
    logger.info(f"ç”Ÿæˆå ±å‘Šå„²å­˜ç›®éŒ„ '{GENERATED_REPORTS_DIR}' å·²ç¢ºèªå­˜åœ¨ã€‚")
    logger.info(f"è³‡æ–™åº«æª”æ¡ˆå°‡å„²å­˜åœ¨ '{DATABASE_URL}'ã€‚")

    init_db() # åˆå§‹åŒ–è³‡æ–™åº«å’Œè¡¨

    # ç’°å¢ƒè®Šæ•¸ä¸­çš„ API é‡‘é‘°å„ªå…ˆæ–¼æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚çš„é…ç½®
    env_api_key = os.getenv("GOOGLE_API_KEY")
    if env_api_key:
        logger.info("[STARTUP] åœ¨ç’°å¢ƒè®Šæ•¸ä¸­æ‰¾åˆ° GOOGLE_API_KEYã€‚å˜—è©¦ä½¿ç”¨å…¶é…ç½® genai...")
        try:
            genai.configure(api_key=env_api_key)
            next(genai.list_models(), None) # é©—è­‰é‡‘é‘°
            logger.info("[STARTUP] [SUCCESS] ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ä¸­çš„ GOOGLE_API_KEY æˆåŠŸé…ç½®ä¸¦é©—è­‰ genaiã€‚")
        except Exception as e_configure:
            logger.error(f"[STARTUP] [ERROR] ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ä¸­çš„ GOOGLE_API_KEY é…ç½® genai æ™‚ç™¼ç”ŸéŒ¯èª¤: {e_configure}")
            # å³ä½¿é€™è£¡å¤±æ•—ï¼Œå¦‚æœç¨å¾Œé€é /api/set_api_key è¨­å®šäº†æœ‰æ•ˆçš„é‡‘é‘°ï¼Œæ‡‰ç”¨ä»å¯èƒ½å·¥ä½œ
    else:
        logger.info("[STARTUP] æœªåœ¨ç’°å¢ƒè®Šæ•¸ä¸­æ‰¾åˆ° GOOGLE_API_KEYã€‚genai å°‡ç­‰å¾… API é‡‘é‘°é€éç«¯é»è¨­å®šã€‚")

    # æ¸…ç†èˆŠçš„è‡¨æ™‚éŸ³è¨Šæª”æ¡ˆ (ç°¡å–®ç¤ºä¾‹ï¼šæ¸…ç†ä¸€å¤©å‰çš„æª”æ¡ˆ)
    # å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œå¯èƒ½éœ€è¦æ›´è¤‡é›œçš„æ¸…ç†ç­–ç•¥æˆ–é€éå¤–éƒ¨æ’ç¨‹ä»»å‹™ä¾†åŸ·è¡Œ
    # æ³¨æ„ï¼šé€™è£¡åªæ¸…ç† TEMP_AUDIO_STORAGE_DIRï¼Œä¸æ¸…ç† GENERATED_REPORTS_DIR
    for filename in os.listdir(TEMP_AUDIO_STORAGE_DIR):
        file_path = os.path.join(TEMP_AUDIO_STORAGE_DIR, filename)
        try:
            if os.path.isfile(file_path) and (datetime.now() - datetime.fromtimestamp(os.path.getmtime(file_path))).days > 1:
                os.remove(file_path)
                logger.info(f"å·²æ¸…ç†éæœŸè‡¨æ™‚æª”æ¡ˆ: {filename}")
        except Exception as e:
            logger.warning(f"æ¸…ç†è‡¨æ™‚æª”æ¡ˆ {filename} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# --- è³‡æ–™åº«è¼”åŠ©å‡½å¼ ---
def get_db_connection():
    os.makedirs(os.path.dirname(DATABASE_URL), exist_ok=True) # ç¢ºä¿ data ç›®éŒ„å­˜åœ¨
    conn = sqlite3.connect(DATABASE_URL, check_same_thread=False) # å…è¨±åœ¨ä¸åŒç·šç¨‹ä¸­ä½¿ç”¨
    conn.row_factory = sqlite3.Row # è®“æŸ¥è©¢çµæœå¯ä»¥åƒå­—å…¸ä¸€æ¨£è¨ªå•åˆ—
    return conn

def init_db():
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS tasks (
            task_id TEXT PRIMARY KEY,
            status TEXT NOT NULL,
            source_name TEXT,
            model_id TEXT,
            submit_time TEXT NOT NULL,
            start_time TEXT,
            completion_time TEXT,
            result_preview_html TEXT,
            download_links TEXT, -- Store as JSON string
            error_message TEXT,
            request_data TEXT     -- Store as JSON string
        )
        ''')
        conn.commit()
        logger.info("SQLite database and tasks table initialized successfully.")
    except sqlite3.Error as e:
        logger.error(f"Error initializing SQLite database: {e}")
        # æ ¹æ“šéœ€è¦ï¼Œé€™è£¡å¯ä»¥æ±ºå®šæ˜¯å¦è¦è®“æ‡‰ç”¨ç¨‹å¼åœ¨è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—æ™‚çµ‚æ­¢
        # raise # é‡æ–°æ‹‹å‡ºç•°å¸¸å¯èƒ½æœƒå°è‡´æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•å¤±æ•—
    except Exception as e_global:
        logger.error(f"An unexpected error occurred during database initialization: {e_global}")
        # raise
    finally:
        if 'conn' in locals() and conn:
            conn.close()

@app.on_event("shutdown")
async def shutdown_event():
    executor.shutdown(wait=True)
    logger.info("[INFO] ThreadPoolExecutor å·²é—œé–‰ã€‚")

# --- API é‡‘é‘°ä¾è³´æ³¨å…¥ ---
async def get_validated_api_key(request: Request) -> str:
    # å„ªå…ˆå¾ç’°å¢ƒè®Šæ•¸è®€å–
    api_key_to_test = os.getenv("GOOGLE_API_KEY")
    source = "environment variable"

    # å¦‚æœç’°å¢ƒè®Šæ•¸æ²’æœ‰ï¼Œå‰‡å˜—è©¦å¾è‡¨æ™‚å­˜å„²è®€å–
    global temporary_api_key_storage
    if not api_key_to_test and temporary_api_key_storage:
        api_key_to_test = temporary_api_key_storage
        source = "temporary storage"

    if not api_key_to_test:
        logger.warning("[API_KEY_DEP] API key not found in environment variables or temporary storage.")
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="API é‡‘é‘°å°šæœªè¨­å®šã€‚è«‹é€éç’°å¢ƒè®Šæ•¸æˆ– API ç«¯é»è¨­å®šã€‚")

    try:
        # åŸºæœ¬æª¢æŸ¥
        if len(api_key_to_test) < 10: # å‡è¨­ API key é•·åº¦è‡³å°‘ç‚º10
            raise ValueError("API key is too short.")

        # é—œéµï¼šä½¿ç”¨æ­¤é‡‘é‘°é…ç½® genai ä¸¦åŸ·è¡Œè¼•é‡ç´šé©—è­‰
        # é€™æ¨£ï¼Œä¾è³´æ­¤å‡½å¼çš„è·¯ç”±å¯ä»¥ç›´æ¥ä½¿ç”¨ genai åŠŸèƒ½ï¼Œè€Œä¸éœ€å†æ¬¡é…ç½®
        genai.configure(api_key=api_key_to_test)
        logger.info(f"[API_KEY_DEP] genai configured with API key from {source}.")
        # å˜—è©¦ list_models ä½œç‚ºæœ€çµ‚é©—è­‰æ­¥é©Ÿ
        next(genai.list_models(), None)
        logger.info(f"[API_KEY_DEP] API key from {source} validated by listing models.")
        return api_key_to_test
    except Exception as e:
        logger.error(f"[API_KEY_DEP] Error during validation or configuration of API key from {source}: {e}")
        # æ¸…é™¤å¯èƒ½å·²è¨­å®šçš„ç„¡æ•ˆè‡¨æ™‚é‡‘é‘°ï¼Œé˜²æ­¢å¾ŒçºŒè«‹æ±‚å˜—è©¦ä½¿ç”¨å®ƒ
        if source == "temporary storage":
            temporary_api_key_storage = None
            logger.info("[API_KEY_DEP] Invalid temporary API key cleared due to validation failure.")
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"æä¾›çš„ API é‡‘é‘° ({source}) ç„¡æ•ˆæˆ–é©—è­‰å¤±æ•—: {e}")

# --- éœæ…‹æª”æ¡ˆèˆ‡ä¸»æ¨¡æ¿ (å¼·åŒ–å•Ÿå‹•æª¢æŸ¥) ---
try:
    # Get the directory where app.py is located
    app_dir = os.path.dirname(os.path.abspath(__file__))
    logger.debug(f"FastAPI app.py å•Ÿå‹•æ™‚çš„ app_dir: {app_dir}")
    static_dir_path = os.path.join(app_dir, "static")
    templates_dir_path = os.path.join(app_dir, templates_main_dir) # templates_main_dir is "templates"

    # åš´æ ¼æª¢æŸ¥ç›®éŒ„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡çµ‚æ­¢æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•
    if not os.path.exists(static_dir_path):
        logger.critical(f"éœæ…‹æª”æ¡ˆç›®éŒ„ '{static_dir_path}' ä¸å­˜åœ¨ã€‚æ‡‰ç”¨ç¨‹å¼ç„¡æ³•å•Ÿå‹•ã€‚")
        sys.exit(1) # å¼·åˆ¶é€€å‡º
    if not os.path.exists(templates_dir_path):
        logger.critical(f"ä¸»æ¨¡æ¿ç›®éŒ„ '{templates_dir_path}' ä¸å­˜åœ¨ã€‚æ‡‰ç”¨ç¨‹å¼ç„¡æ³•å•Ÿå‹•ã€‚")
        sys.exit(1) # å¼·åˆ¶é€€å‡º

    app.mount("/static", StaticFiles(directory=static_dir_path), name="static")
    templates = Jinja2Templates(directory=templates_dir_path)
except Exception as e:
    logger.critical(f"è¨­å®šéœæ…‹æª”æ¡ˆæˆ–ä¸»æ¨¡æ¿æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}ã€‚æ‡‰ç”¨ç¨‹å¼ç„¡æ³•å•Ÿå‹•ã€‚")
    sys.exit(1) # å¼·åˆ¶é€€å‡º


# --- API é‡‘é‘°ç®¡ç† API (å¼·åŒ–é©—è­‰é‚è¼¯) ---
@app.post("/api/set_api_key")
async def set_api_key(request_data: SetApiKeyRequest):
    global temporary_api_key_storage
    logger.debug(f"Received request to set temporary API key.")
    try:
        # Validate and configure genai with the new key
        genai.configure(api_key=request_data.api_key)
        next(genai.list_models(), None) # Validate by trying to list models

        temporary_api_key_storage = request_data.api_key
        logger.info(f"[SUCCESS] Temporary API key set and validated successfully.")
        return {"message": "è‡¨æ™‚ API é‡‘é‘°å·²è¨­å®šä¸¦é©—è­‰æˆåŠŸã€‚"}
    except Exception as e_val:
        logger.error(f"[ERROR] Failed to validate or set temporary API key: {e_val}")
        # temporary_api_key_storage = None # Ensure it's not set if validation fails - get_validated_api_key handles this
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"æä¾›çš„è‡¨æ™‚ API é‡‘é‘°é©—è­‰å¤±æ•—: {e_val}")

@app.get("/api/check_api_key_status")
async def check_api_key_status(api_key: str = Depends(get_validated_api_key)):
    # If get_validated_api_key dependency succeeds, it means an API key (either from env or temp)
    # has been found and successfully used to configure and validate genai.
    # The actual key string is in api_key variable but we don't need to use it here explicitly.
    env_key_exists = bool(os.getenv("GOOGLE_API_KEY"))
    temp_key_exists = bool(temporary_api_key_storage)

    status_detail = "API é‡‘é‘°æœ‰æ•ˆã€‚"
    if env_key_exists:
        status_detail += " (ä¾†æºï¼šç’°å¢ƒè®Šæ•¸)"
    elif temp_key_exists:
        status_detail += " (ä¾†æºï¼šè‡¨æ™‚è¨­å®š)"
    else:
        # This case should ideally not be reached if get_validated_api_key works correctly,
        # as it would raise an exception if no key is found.
        status_detail = "API é‡‘é‘°æœ‰æ•ˆï¼Œä½†ç„¡æ³•åˆ¤æ–·ä¾†æº (ç•°å¸¸æƒ…æ³)ã€‚"

    logger.debug(f"API é‡‘é‘°ç‹€æ…‹æª¢æŸ¥ï¼š{status_detail}")
    return {"status": "set_and_valid", "message": status_detail}
    # --- Old logic commented out ---
    # global global_api_key, api_key_is_valid
    # if global_api_key and api_key_is_valid:
    #     logger.debug("API é‡‘é‘°ç‹€æ…‹æª¢æŸ¥ï¼šå·²è¨­å®šä¸”æœ‰æ•ˆã€‚")
    #     return {"status": "set_and_valid", "message": "API é‡‘é‘°å·²è¨­å®šä¸”æœ‰æ•ˆã€‚"}
    # elif global_api_key and not api_key_is_valid:
    #     logger.debug("API é‡‘é‘°ç‹€æ…‹æª¢æŸ¥ï¼šå·²è¨­å®šä½†é©—è­‰å¤±æ•—ã€‚")
    #     return {"status": "set_but_invalid", "message": "API é‡‘é‘°å·²è¨­å®šä½†é©—è­‰å¤±æ•—ï¼Œè«‹é‡æ–°è¨­å®šã€‚"}
    # else:
    #     logger.debug("API é‡‘é‘°ç‹€æ…‹æª¢æŸ¥ï¼šæœªè¨­å®šã€‚")
    #     return {"status": "not_set", "message": "API é‡‘é‘°å°šæœªè¨­å®šï¼Œè«‹è¨­å®š API é‡‘é‘°ã€‚"}


# --- è¼”åŠ©å‡½å¼ (sanitize_filename, generate_html_report_content_via_jinja) ---
def sanitize_base_filename(title: str, max_length: int = 50) -> str:
    if not title: title = "untitled_audio"
    title = str(title)
    title = re.sub(r'[\\/:*?"<>|&]', "_", title)
    title = title.replace(" ", "_")
    title = re.sub(r"_+", "_", title)
    title = title.strip('_')
    if title.startswith('.'): title = "_" + title[1:]
    return title[:max_length]

def generate_html_report_content_via_jinja(report_title: str, summary_data: Optional[Dict], transcript_data: Optional[Dict], model_id: str) -> str:
    try:
        return report_content_jinja_template.render(
            report_title=report_title,
            summary_data=summary_data,
            transcript_paragraphs=transcript_data,
            model_id=model_id
        )
    except Exception as e:
        logger.error(f"Jinja2 æ¨¡æ¿æ¸²æŸ“å ±å‘Šå…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        traceback.print_exc()
        return f"<div class='report-content'><p style='color:red;'>æŠ±æ­‰ï¼Œç”Ÿæˆå ±å‘Šé è¦½æ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤ï¼š{str(e)}</p></div>"

# --- process_audio_and_generate_report_task (å¼·åŒ–éŒ¯èª¤è™•ç†) ---
async def process_audio_and_generate_report_task(task_id: str, request_data: GenerateReportRequest, api_key: str):
    # api_key is passed from the route that used Depends(get_validated_api_key)
    # Configure genai for this background task execution context
    try:
        genai.configure(api_key=api_key)
        # Optional: further validation like list_models() if desired, but get_validated_api_key should have done it.
        logger.info(f"[TASK {task_id}] genai configured successfully for background execution.")
    except Exception as e_conf_bg:
        logger.error(f"[TASK {task_id}] [ERROR_CRITICAL] Failed to configure genai in background task: {e_conf_bg}")
        # Update task status to failed
        error_message = f"èƒŒæ™¯ä»»å‹™ä¸­ API é‡‘é‘°é…ç½®å¤±æ•—: {e_conf_bg}"
        completion_time_iso = datetime.now(timezone.utc).isoformat()
        try:
            conn_err = get_db_connection()
            cursor_err = conn_err.cursor()
            cursor_err.execute("UPDATE tasks SET status = ?, error_message = ?, completion_time = ? WHERE task_id = ?",
                               ("failed", error_message, completion_time_iso, task_id))
            conn_err.commit()
        except sqlite3.Error as e_sql_err:
            logger.error(f"[TASK {task_id}] [ERROR_DB] æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚º 'failed' (èƒŒæ™¯é‡‘é‘°é…ç½®éŒ¯èª¤) æ™‚ SQLite éŒ¯èª¤: {e_sql_err}")
        finally:
            if 'conn_err' in locals() and conn_err: conn_err.close()
        return # Critical failure, cannot proceed

    start_time_iso = datetime.now(timezone.utc).isoformat()
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("UPDATE tasks SET status = ?, start_time = ? WHERE task_id = ?",
                       ("processing", start_time_iso, task_id))
        conn.commit()
        logger.info(f"[TASK {task_id}] ç‹€æ…‹æ›´æ–°ç‚º 'processing', é–‹å§‹æ™‚é–“: {start_time_iso}")
    except sqlite3.Error as e_sql:
        logger.error(f"[TASK {task_id}] [ERROR_DB] æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚º 'processing' æ™‚ SQLite éŒ¯èª¤: {e_sql}")
        # å¦‚æœåˆå§‹ç‹€æ…‹æ›´æ–°å¤±æ•—ï¼Œå¯èƒ½éœ€è¦æ±ºå®šæ˜¯å¦ç¹¼çºŒä»»å‹™
        # æ­¤è™•é¸æ“‡ç¹¼çºŒï¼Œä½†è¨˜éŒ„éŒ¯èª¤
    finally:
        if 'conn' in locals() and conn: conn.close()

    logger.info(f"[TASK {task_id}] è™•ç†é–‹å§‹: {request_data.source_path} (æ¨¡å‹: {request_data.model_id})")

    # Removed check for global_api_key and api_key_is_valid, as api_key is now passed and configured.
    # if not global_api_key or not api_key_is_valid:
    #     error_message = "API é‡‘é‘°ç„¡æ•ˆæˆ–æœªè¨­å®šæ–¼èƒŒæ™¯ä»»å‹™å•Ÿå‹•æ™‚ã€‚è«‹å…ˆè¨­å®šæœ‰æ•ˆçš„ API é‡‘é‘°ã€‚"
    #     completion_time_iso = datetime.now(timezone.utc).isoformat()
    #     try:
    #         conn = get_db_connection()
    #         cursor = conn.cursor()
    #         cursor.execute("UPDATE tasks SET status = ?, error_message = ?, completion_time = ? WHERE task_id = ?",
    #                        ("failed", error_message, completion_time_iso, task_id))
    #         conn.commit()
    #     except sqlite3.Error as e_sql_fail:
    #         logger.error(f"[TASK {task_id}] [ERROR_DB] æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚º 'failed' (API é‡‘é‘°ç„¡æ•ˆ) æ™‚ SQLite éŒ¯èª¤: {e_sql_fail}")
    #     finally:
    #         if 'conn' in locals() and conn: conn.close()
    #     logger.error(f"[TASK {task_id}] [ERROR] å¤±æ•— - {error_message}"); return

    try:
        # æ¨¡æ“¬ AI è¼¸å‡ºå’Œçµæ§‹åŒ–è³‡æ–™è½‰æ›é‚è¼¯
        # é€™è£¡æ‡‰è©²æ˜¯å¯¦éš›èª¿ç”¨ genai API çš„åœ°æ–¹
        # æ³¨æ„ï¼šä¹‹å‰ä½¿ç”¨ len(tasks_db) * 0.5 çš„æ¨¡æ“¬å»¶æ™‚ï¼Œç¾åœ¨ tasks_db ä¸å†æ˜¯è¨˜æ†¶é«”å­—å…¸ã€‚
        # å¦‚æœéœ€è¦åŸºæ–¼ç•¶å‰ä»»å‹™æ•¸é‡çš„å»¶æ™‚ï¼Œéœ€è¦å¾è³‡æ–™åº«æŸ¥è©¢ã€‚ç‚ºç°¡åŒ–ï¼Œé€™è£¡ä½¿ç”¨å›ºå®šå»¶æ™‚ã€‚
        await asyncio.sleep(5 + 2 * 0.5) # å‡è¨­å¹³å‡æœ‰2å€‹ä»»å‹™åœ¨é‹è¡Œ

        # --- æŒ‡å°åŸå‰‡ï¼šè™•ç†åŒæ­¥é˜»å¡çš„ AI å‘¼å« ---
        # æœªä¾†å°‡æ­¤è™•çš„æ¨¡æ“¬æ“ä½œæ›¿æ›ç‚ºå¯¦éš›çš„ AI æ¨¡å‹å‘¼å« (ä¾‹å¦‚ google-generativeai)ã€‚
        # ç”±æ–¼ genai.GenerativeModel().generate_content() æ˜¯åŒæ­¥é˜»å¡æ“ä½œï¼Œ
        # å¿…é ˆåœ¨ asyncio äº‹ä»¶è¿´åœˆä¸­é€é await asyncio.to_thread() æˆ–
        # await loop.run_in_executor(executor, ...) ä¾†åŸ·è¡Œï¼Œä»¥é¿å…é˜»å¡ä¼ºæœå™¨ã€‚
        # ç¯„ä¾‹:
        # model = genai.GenerativeModel(request_data.model_id) # å‡è¨­ genai å·²é…ç½® API Key
        #
        # # ä½¿ç”¨ asyncio.to_thread (Python 3.9+)
        # # response_summary = await asyncio.to_thread(model.generate_content, "æç¤ºè©æ‘˜è¦: " + source_basename)
        # # simulated_summary_text = response_summary.text
        #
        # # æˆ–è€…ä½¿ç”¨ loop.run_in_executor (éœ€è¦ç²å– loop å’Œ executor)
        # # loop = asyncio.get_running_loop()
        # # response_transcript = await loop.run_in_executor(executor, model.generate_content, "æç¤ºè©é€å­—ç¨¿: " + source_basename)
        # # simulated_transcript_text = response_transcript.text
        #
        # # è«‹ç¢ºä¿ genai.configure(api_key=...) å·²åœ¨æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚æˆ–é€éä¾è³´æ³¨å…¥æœ‰æ•ˆåŸ·è¡Œã€‚
        # --- çµæŸæŒ‡å°åŸå‰‡ ---

        simulated_summary_text: Optional[str] = None
        simulated_transcript_text: Optional[str] = None
        source_basename = os.path.basename(request_data.source_path)

        # æ ¹æ“š output_options å’Œ custom_prompts ç”Ÿæˆæ¨¡æ“¬å…§å®¹
        # é€™è£¡å¯ä»¥åŠ å…¥ genai.GenerativeModel().generate_content() çš„å¯¦éš›å‘¼å«
        # ç¤ºä¾‹ï¼š
        # model = genai.GenerativeModel(request_data.model_id)
        # response = model.generate_content("è«‹ç¸½çµé€™æ®µéŸ³è¨Šçš„å…§å®¹ï¼š" + source_basename, stream=False)
        # simulated_summary_text = response.text

        if "summary_tc" in request_data.output_options or "summary_transcript_tc" in request_data.output_options or "transcript_bilingual_summary" in request_data.output_options:
            base_summary = f"é€™æ˜¯å° '{source_basename}' ä½¿ç”¨ '{request_data.model_id}' æ¨¡å‹ç”Ÿæˆçš„ç¹é«”ä¸­æ–‡é‡é»æ‘˜è¦çš„é–‹é ­ç¸½çµæ®µè½ã€‚\n" \
                           f"**é‡é»1å­æ¨™é¡Œ**\n- é€™æ˜¯ç¬¬ä¸€å€‹é‡é»çš„ç¬¬ä¸€å€‹ç´°ç¯€ã€‚\n- é€™æ˜¯ç¬¬ä¸€å€‹é‡é»çš„ç¬¬äºŒå€‹ç´°ç¯€ã€‚\n" \
                           f"**é‡é»2å­æ¨™é¡Œ**\n- é€™æ˜¯ç¬¬äºŒå€‹é‡é»çš„ç¬¬ä¸€å€‹ç´°ç¯€ï¼Œå®ƒå¯èƒ½æ¯”è¼ƒé•·ä¸€é»ã€‚\n- é€™æ˜¯ç¬¬äºŒå€‹é‡é»çš„ç¬¬äºŒå€‹ç´°ç¯€ã€‚"
            if request_data.custom_prompts and request_data.custom_prompts.get("summary_prompt"):
                base_summary = f"æ ¹æ“šè‡ªè¨‚æç¤ºè© '{request_data.custom_prompts['summary_prompt'][:50]}...' ç”Ÿæˆçš„æ‘˜è¦ï¼š\n" + base_summary
            simulated_summary_text = base_summary
            if "transcript_bilingual_summary" in request_data.output_options: simulated_summary_text += "\n(This is the English part of the bilingual summary.)"

        if "summary_transcript_tc" in request_data.output_options or "transcript_bilingual_summary" in request_data.output_options:
            base_transcript = f"é€™æ˜¯ '{source_basename}' çš„æ¨¡æ“¬é€å­—ç¨¿å…§å®¹çš„ç¬¬ä¸€æ®µã€‚\n" \
                              f"é€™æ˜¯ç¬¬äºŒæ®µã€‚\nç™¼è¨€è€…Aï¼šæ¨¡æ“¬å°è©±é–‹å§‹ã€‚\nç™¼è¨€è€…Bï¼šå¥½çš„ã€‚\nç¬¬äº”æ®µï¼Œè§¸ç™¼åˆ†éš”ç·šã€‚\nç¬¬å…­æ®µã€‚"
            if request_data.custom_prompts and request_data.custom_prompts.get("transcript_prompt"):
                base_transcript = f"æ ¹æ“šè‡ªè¨‚æç¤ºè© '{request_data.custom_prompts['transcript_prompt'][:50]}...' ç”Ÿæˆçš„é€å­—ç¨¿ï¼š\n" + base_transcript
            simulated_transcript_text = base_transcript
            if "transcript_bilingual_summary" in request_data.output_options: simulated_transcript_text = f"(Original Language Transcript for '{source_basename}')\n" + simulated_transcript_text

        # è½‰æ›ç‚ºçµæ§‹åŒ–è³‡æ–™
        structured_summary_data = None
        if simulated_summary_text:
            lines = simulated_summary_text.strip().split('\n')
            intro = lines.pop(0) if lines else ""
            bilingual_append_text = None
            if "transcript_bilingual_summary" in request_data.output_options and lines and "(This is the English part" in lines[-1]:
                bilingual_append_text = lines.pop(-1)
            items = []
            current_item_details = []
            current_subtitle = None
            for line in lines:
                if line.startswith("**") and line.endswith("**"):
                    if current_subtitle:
                        items.append({"subtitle": current_subtitle.strip('*'), "details": list(current_item_details)})
                    current_subtitle = line.strip('*')
                    current_item_details.clear()
                elif line.startswith("- ") and current_subtitle:
                    current_item_details.append(line[2:])
                elif current_subtitle and current_item_details: # è™•ç†å¤šè¡Œç´°ç¯€
                    current_item_details[-1] += "\n" + line
                elif current_subtitle: # è™•ç†æ²’æœ‰ - é–‹é ­çš„ç´°ç¯€
                    current_item_details.append(line)

            if current_subtitle: # æ·»åŠ æœ€å¾Œä¸€å€‹é …ç›®
                items.append({"subtitle": current_subtitle.strip('*'), "details": list(current_item_details)})
            structured_summary_data = {"intro_paragraph": intro, "items": items, "bilingual_append": bilingual_append_text}

        structured_transcript_data = None
        if simulated_transcript_text:
            paragraphs_raw = simulated_transcript_text.strip().split('\n')
            hr_interval = 5
            formatted_paragraphs = []
            bilingual_prepend_text = None
            if "transcript_bilingual_summary" in request_data.output_options and paragraphs_raw and paragraphs_raw[0].startswith("(Original Language Transcript"):
                bilingual_prepend_text = paragraphs_raw.pop(0)
            for i, p_text in enumerate(paragraphs_raw):
                if p_text.strip():
                    match = re.match(r"^(ç™¼è¨€è€…\s?[A-Za-z0-9]+)[:ï¼š]\s*(.*)", p_text)
                    formatted_paragraphs.append({
                        "content": match.group(2) if match else p_text,
                        "is_speaker_line": bool(match),
                        "speaker": match.group(1) if match else None,
                        "insert_hr_after": (i + 1) % hr_interval == 0 and i < len(paragraphs_raw) - 1
                    })
            structured_transcript_data = {"bilingual_prepend": bilingual_prepend_text, "paragraphs": formatted_paragraphs}

        # æª”æ¡ˆç”Ÿæˆé‚è¼¯
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("UPDATE tasks SET status = ? WHERE task_id = ?", ("generating_report", task_id))
            conn.commit()
            logger.info(f"[TASK {task_id}] ç‹€æ…‹æ›´æ–°ç‚º 'generating_report'")
        except sqlite3.Error as e_sql_gen_report_status:
            logger.warning(f"[TASK {task_id}] [ERROR_DB] æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚º 'generating_report' æ™‚ SQLite éŒ¯èª¤: {e_sql_gen_report_status}")
        finally:
            if 'conn' in locals() and conn: conn.close()

        await asyncio.sleep(1) # æ¨¡æ“¬ç”Ÿæˆå ±å‘Šçš„æ™‚é–“

        report_title = f"'{source_basename}' çš„ AI åˆ†æå ±å‘Š"
        preview_html = generate_html_report_content_via_jinja(report_title, structured_summary_data, structured_transcript_data, request_data.model_id)

        base_report_filename = f"{sanitize_base_filename(source_basename, 30)}_{request_data.model_id}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        download_links = {}

        # ç”Ÿæˆå®Œæ•´çš„ HTML å ±å‘Š
        full_html_content = f"""<!DOCTYPE html><html lang="zh-Hant"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>{report_title}</title><link rel="stylesheet" href="/static/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"></head><body class="dark-mode"><div class="container content-wrapper" style="padding-top: 20px; padding-bottom: 20px;"><section class="card animated" style="margin-bottom:0;">{preview_html}</section></div></body></html>"""
        html_file_path = os.path.join(GENERATED_REPORTS_DIR, f"{base_report_filename}.html")
        try:
            with open(html_file_path, "w", encoding="utf-8") as f:
                f.write(full_html_content)
            download_links["html"] = f"/generated_reports/{os.path.basename(html_file_path)}"
        except Exception as e:
            logger.error(f"[TASK {task_id}] [ERROR] å„²å­˜ HTML å ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        # ç”Ÿæˆ Markdown å ±å‘Š
        if "md" in request_data.output_options:
            md_parts = [f"# {report_title}\n\n"]
            if structured_summary_data:
                md_parts.extend(
                    [f"## é‡é»æ‘˜è¦\n\n{structured_summary_data['intro_paragraph']}\n\n" if structured_summary_data.get('intro_paragraph') else "## é‡é»æ‘˜è¦\n\n"] +
                    [f"### {item['subtitle']}\n" + "".join([f"- {d}\n" for d in item.get('details', [])]) + "\n" for item in structured_summary_data.get('items', [])] +
                    ([f"{structured_summary_data['bilingual_append']}\n\n"] if structured_summary_data.get('bilingual_append') else [])
                )
            if structured_transcript_data:
                md_parts.extend(
                    ["## é€å­—ç¨¿\n\n"] +
                    ([f"{structured_transcript_data['bilingual_prepend']}\n\n"] if structured_transcript_data.get('bilingual_prepend') else []) +
                    [f"**{p['speaker']}:** {p['content']}\n\n" if p["is_speaker_line"] else f"{p['content']}\n\n" for p in structured_transcript_data.get('paragraphs', [])]
                )
            md_file_path = os.path.join(GENERATED_REPORTS_DIR, f"{base_report_filename}.md")
            try:
                with open(md_file_path, "w", encoding="utf-8") as f:
                    f.write("".join(md_parts))
                download_links["md"] = f"/generated_reports/{os.path.basename(md_file_path)}"
            except Exception as e:
                logger.error(f"[TASK {task_id}] [ERROR] å„²å­˜ Markdown å ±å‘ŠéŒ¯èª¤: {e}")

        # ç”Ÿæˆ TXT å ±å‘Š
        if "txt" in request_data.output_options:
            txt_parts = [f"{report_title}\n\n"]
            if structured_summary_data:
                txt_parts.extend(
                    ["é‡é»æ‘˜è¦\n--------------------\n"] +
                    ([f"{structured_summary_data['intro_paragraph']}\n\n"] if structured_summary_data.get('intro_paragraph') else []) +
                    [f"{item['subtitle']}\n" + "".join([f"  - {d}\n" for d in item.get('details', [])]) + "\n" for item in structured_summary_data.get('items', [])] +
                    ([f"{structured_summary_data['bilingual_append']}\n\n"] if structured_summary_data.get('bilingual_append') else [])
                )
            if structured_transcript_data:
                txt_parts.extend(
                    ["é€å­—ç¨¿\n--------------------\n"] +
                    ([f"{structured_transcript_data['bilingual_prepend']}\n\n"] if structured_transcript_data.get('bilingual_prepend') else []) +
                    [f"{p['speaker']}: {p['content']}\n\n" if p["is_speaker_line"] else f"{p['content']}\n\n" for p in structured_transcript_data.get('paragraphs', [])]
                )
            txt_file_path = os.path.join(GENERATED_REPORTS_DIR, f"{base_report_filename}.txt")
            try:
                with open(txt_file_path, "w", encoding="utf-8") as f:
                    f.write("".join(txt_parts))
                download_links["txt"] = f"/generated_reports/{os.path.basename(txt_file_path)}"
            except Exception as e:
                logger.error(f"[TASK {task_id}] [ERROR] å„²å­˜ TXT å ±å‘ŠéŒ¯èª¤: {e}")

        completion_time_iso = datetime.now(timezone.utc).isoformat()
        download_links_json = json.dumps(download_links)
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute(
                "UPDATE tasks SET status = ?, result_preview_html = ?, download_links = ?, completion_time = ? WHERE task_id = ?",
                ("completed", preview_html, download_links_json, completion_time_iso, task_id)
            )
            conn.commit()
            logger.info(f"[TASK {task_id}] [SUCCESS] è™•ç†å®Œæˆã€‚çµæœå·²å­˜å…¥è³‡æ–™åº«ã€‚")
        except sqlite3.Error as e_sql_complete:
            logger.error(f"[TASK {task_id}] [ERROR_DB] æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚º 'completed' ä¸¦å„²å­˜çµæœæ™‚ SQLite éŒ¯èª¤: {e_sql_complete}")
            # å³ä½¿è³‡æ–™åº«æ›´æ–°å¤±æ•—ï¼Œä»»å‹™å¯¦éš›ä¸Šå¯èƒ½å·²å®Œæˆï¼Œä½†ç‹€æ…‹æœªæ­£ç¢ºåæ˜ 
        finally:
            if 'conn' in locals() and conn: conn.close()

    except Exception as e:
        error_message = f"èƒŒæ™¯ä»»å‹™è™•ç†å¤±æ•—: {str(e)}"
        completion_time_iso = datetime.now(timezone.utc).isoformat()
        logger.error(f"[TASK {task_id}] [ERROR] {error_message}")
        traceback.print_exc()
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("UPDATE tasks SET status = ?, error_message = ?, completion_time = ? WHERE task_id = ?",
                           ("failed", error_message, completion_time_iso, task_id))
            conn.commit()
        except sqlite3.Error as e_sql_final_fail:
            logger.error(f"[TASK {task_id}] [ERROR_DB] æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚º 'failed' (ä¸€èˆ¬éŒ¯èª¤) æ™‚ SQLite éŒ¯èª¤: {e_sql_final_fail}")
        finally:
            if 'conn' in locals() and conn: conn.close()


# --- éŸ³è¨Šä¾†æºè™•ç† API ---
def _download_youtube_audio_sync(youtube_url: str, task_id_for_log: Optional[str]="N/A") -> str:
    logger.info(f"[TASK {task_id_for_log}] [SYNC_DOWNLOAD] é–‹å§‹ä¸‹è¼‰ YouTube éŸ³è¨Š: {youtube_url}")
    try:
        yt = YouTube(youtube_url)
        audio_stream = yt.streams.get_audio_only()
        if not audio_stream:
            audio_stream = yt.streams.filter(only_audio=True, file_extension='m4a').order_by('abr').desc().first()
        if not audio_stream:
            audio_stream = yt.streams.filter(only_audio=True, file_extension='webm').order_by('abr').desc().first()
        if not audio_stream:
            raise PytubeFixError(f"åœ¨ YouTube å½±ç‰‡ '{youtube_url}' ä¸­æ‰¾ä¸åˆ°åˆé©çš„éŸ³è¨Šæµã€‚")

        title_part = sanitize_base_filename(yt.title, max_length=30)
        timestamp_part = datetime.now().strftime("%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        file_extension = audio_stream.subtype if audio_stream.subtype else "mp4"
        final_filename = f"{title_part}_{timestamp_part}_{unique_id}.{file_extension}"
        actual_downloaded_path = audio_stream.download(output_path=TEMP_AUDIO_STORAGE_DIR, filename=final_filename)

        if not os.path.exists(actual_downloaded_path) or os.path.getsize(actual_downloaded_path) == 0:
            raise PytubeFixError(f"YouTube éŸ³è¨Šæª”æ¡ˆ '{final_filename}' ä¸‹è¼‰å¾Œæœªæ‰¾åˆ°æˆ–ç‚ºç©ºã€‚")

        logger.info(f"[TASK {task_id_for_log}] [SYNC_DOWNLOAD] [SUCCESS] YouTube éŸ³è¨ŠæˆåŠŸä¸‹è¼‰è‡³: {actual_downloaded_path}")
        return actual_downloaded_path
    except PytubeFixError as pte:
        error_msg = f"PytubeFix åœ¨è™•ç† YouTube éŸ³è¨Š '{youtube_url}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(pte)}"
        logger.error(f"[TASK {task_id_for_log}] [SYNC_DOWNLOAD] [ERROR] {error_msg}")
        traceback.print_exc()
        raise PytubeFixError(error_msg) from pte
    except Exception as e:
        error_msg = f"ä¸‹è¼‰ YouTube éŸ³è¨Š '{youtube_url}' æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {str(e)}"
        logger.error(f"[TASK {task_id_for_log}] [SYNC_DOWNLOAD] [ERROR] {error_msg}")
        traceback.print_exc()
        raise RuntimeError(error_msg) from e

@app.post("/api/process_youtube_url", response_model=Dict[str, str])
async def api_process_youtube_url(request_data: ProcessUrlRequest):
    log_task_id = str(uuid.uuid4())[:8]
    logger.info(f"[API_YT_URL] [TASK {log_task_id}] æ”¶åˆ° YouTube ç¶²å€è™•ç†è«‹æ±‚: {request_data.url}")
    # Pydantic å·²ç¶“åšäº† URL æ ¼å¼é©—è­‰ï¼Œé€™è£¡å¯ä»¥ç°¡åŒ–
    try:
        loop = asyncio.get_event_loop()
        local_audio_path = await loop.run_in_executor(executor, _download_youtube_audio_sync, request_data.url, log_task_id)
        return {"message": f"YouTube éŸ³è¨Š '{os.path.basename(local_audio_path)}' å·²æˆåŠŸä¸‹è¼‰è‡³ä¼ºæœå™¨ã€‚", "youtube_url": request_data.url, "processed_audio_path": local_audio_path}
    except PytubeFixError as pte:
        raise HTTPException(status_code=500, detail=str(pte))
    except Exception as e:
        logger.error(f"[API_YT_URL] [TASK {log_task_id}] [ERROR] è™•ç† YouTube ç¶²å€æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è™•ç† YouTube ç¶²å€æ™‚ç™¼ç”Ÿä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {str(e)}")

@app.post("/api/upload_audio_file", response_model=Dict[str, str])
async def api_upload_audio_file(audio_file: UploadFile = File(...)):
    log_task_id = str(uuid.uuid4())[:8]
    logger.info(f"[API_UPLOAD] [TASK {log_task_id}] æ”¶åˆ°æª”æ¡ˆä¸Šå‚³: {audio_file.filename}, é¡å‹: {audio_file.content_type}")
    original_name, original_ext = os.path.splitext(audio_file.filename if audio_file.filename else "uploaded_audio")

    # å˜—è©¦å¾ content_type æ¨æ–·å‰¯æª”å
    if not original_ext and audio_file.content_type:
        content_type_map = {
            "audio/mpeg": ".mp3", "audio/mp4": ".m4a", "audio/webm": ".webm",
            "audio/wav": ".wav", "audio/ogg": ".ogg", "audio/aac": ".aac"
        }
        original_ext = content_type_map.get(audio_file.content_type, ".bin") # é è¨­ç‚º .bin ä»¥é˜²è¬ä¸€

    base_name = sanitize_base_filename(original_name, max_length=30)
    timestamp = datetime.now().strftime("%m%d_%H%M%S")
    unique_id = str(uuid.uuid4())[:8]
    temp_upload_filename = f"{base_name}_{timestamp}_{unique_id}{original_ext}"
    temp_upload_path = os.path.join(TEMP_AUDIO_STORAGE_DIR, temp_upload_filename)

    try:
        # ä½¿ç”¨ asyncio.to_thread ç¢ºä¿æª”æ¡ˆå¯«å…¥ä¸æœƒé˜»å¡ FastAPI äº‹ä»¶å¾ªç’°
        await asyncio.to_thread(shutil.copyfileobj, audio_file.file, open(temp_upload_path, "wb"))
        logger.info(f"[API_UPLOAD] [TASK {log_task_id}] [SUCCESS] æª”æ¡ˆæˆåŠŸå„²å­˜è‡³: {temp_upload_path} (å¤§å°: {os.path.getsize(temp_upload_path)} bytes)")
        return {"message": f"éŸ³è¨Šæª”æ¡ˆ '{audio_file.filename}' ä¸Šå‚³ä¸¦å„²å­˜æˆåŠŸã€‚", "filename": audio_file.filename, "content_type": audio_file.content_type, "processed_audio_path": temp_upload_path}
    except Exception as e:
        logger.error(f"[API_UPLOAD] [TASK {log_task_id}] [ERROR] å„²å­˜ä¸Šå‚³æª”æ¡ˆå¤±æ•—: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è™•ç†ä¸Šå‚³æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    finally:
        await audio_file.close()


# --- AI æ¨¡å‹èˆ‡å ±å‘Šç”Ÿæˆ API ---

# é å®šç¾©æ¨¡å‹è³‡æ–™ (åŒ…å«ä¸­æ–‡ä»‹ç´¹å’Œæ’åºè³‡è¨Š)
PREDEFINED_MODELS_DATA_APP = {
    "models/gemini-1.5-flash-latest": {
        "id": "models/gemini-1.5-flash-latest",
        "dropdown_display_name": "âš¡ Gemini 1.5 Flash (æœ€æ–°ç‰ˆ)",
        "chinese_display_name": "Gemini 1.5 Flash æœ€æ–°ç‰ˆ",
        "chinese_summary_parenthesized": "ï¼ˆé€Ÿåº¦å¿«ã€å¤šåŠŸèƒ½ã€å¤šæ¨¡æ…‹ã€é©ç”¨æ–¼å¤šæ¨£åŒ–ä»»å‹™æ“´å±•ï¼‰",
        "chinese_input_output": "è¼¸å…¥ï¼šæ–‡å­—ã€éŸ³è¨Šã€åœ–ç‰‡ã€å½±ç‰‡ (è©³è¦‹å®˜æ–¹æ–‡ä»¶)ï¼›è¼¸å‡ºï¼šæ–‡å­—",
        "chinese_suitable_for": "éœ€è¦å¿«é€Ÿå›æ‡‰çš„å¤šæ¨£åŒ–ä»»å‹™ã€å¤§è¦æ¨¡æ‡‰ç”¨ã€èŠå¤©ã€æ‘˜è¦ã€éŸ³è¨Šè™•ç†ã€‚",
        "original_description_from_api": "Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.",
        "sort_priority": 1 # æ’åºå„ªå…ˆç´šï¼Œæ•¸å­—è¶Šå°è¶Šé å‰
    },
    "models/gemini-1.5-pro-latest": {
        "id": "models/gemini-1.5-pro-latest",
        "dropdown_display_name": "ğŸŒŸ Gemini 1.5 Pro (æœ€æ–°ç‰ˆ)",
        "chinese_display_name": "Gemini 1.5 Pro æœ€æ–°ç‰ˆ",
        "chinese_summary_parenthesized": "ï¼ˆåŠŸèƒ½å¼·å¤§ã€å¤§å‹ä¸Šä¸‹æ–‡ã€å¤šæ¨¡æ…‹ã€ç†è§£è¤‡é›œæƒ…å¢ƒï¼‰",
        "chinese_input_output": "è¼¸å…¥ï¼šæ–‡å­—ã€éŸ³è¨Šã€åœ–ç‰‡ã€å½±ç‰‡ (æœ€é«˜é”2ç™¾è¬tokenï¼Œè©³è¦‹å®˜æ–¹æ–‡ä»¶)ï¼›è¼¸å‡ºï¼šæ–‡å­—",
        "chinese_suitable_for": "è¤‡é›œæ¨ç†ã€é•·ç¯‡å…§å®¹ç†è§£èˆ‡ç”Ÿæˆã€å¤šæ¨¡æ…‹åˆ†æã€ç¨‹å¼ç¢¼ç”Ÿæˆèˆ‡è§£é‡‹ã€‚",
        "original_description_from_api": "Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.",
        "sort_priority": 0 # æœ€é«˜å„ªå…ˆç´š
    },
    "models/gemini-pro": { # å‡è¨­é€™æ˜¯ç´”æ–‡å­—çš„ gemini-pro
        "id": "models/gemini-pro",
        "dropdown_display_name": "Gemini Pro (ç´”æ–‡å­—)",
        "chinese_display_name": "Gemini Pro (ç´”æ–‡å­—ç‰ˆ)",
        "chinese_summary_parenthesized": "ï¼ˆå„ªåŒ–çš„ç´”æ–‡å­—ç”Ÿæˆèˆ‡ç†è§£ï¼‰",
        "chinese_input_output": "è¼¸å…¥ï¼šæ–‡å­—ï¼›è¼¸å‡ºï¼šæ–‡å­—",
        "chinese_suitable_for": "ç´”æ–‡å­—çš„å•ç­”ã€æ‘˜è¦ã€å¯«ä½œã€ç¿»è­¯ç­‰ã€‚",
        "original_description_from_api": "Optimized for text-only prompts.",
        "sort_priority": 10
    }
    # æ‚¨å¯ä»¥æ ¹æ“šéœ€è¦åŠ å…¥æ›´å¤šé å®šç¾©æ¨¡å‹å’Œå®ƒå€‘çš„ä¸­æ–‡è³‡è¨Š
}

def get_model_version_score(api_name_lower: str) -> int:
    score = 9999 # é è¨­è¼ƒä½å„ªå…ˆç´š
    if "latest" in api_name_lower: score = 0
    elif "preview" in api_name_lower:
        score = 1000
        date_match = re.search(r'preview[_-](\d{2})[_-]?(\d{2})', api_name_lower) # ä¾‹å¦‚ preview-0527
        if date_match:
            try:
                # çµ„åˆæ—¥æœŸï¼Œä¾‹å¦‚ 0527 -> 527ï¼Œæ•¸å­—è¶Šå¤§ä»£è¡¨æ—¥æœŸè¶Šæ–°
                date_score = int(date_match.group(1)) * 100 + int(date_match.group(2))
                score -= date_score # æ•¸å­—è¶Šå¤§è¶Šæ–°ï¼Œæ‰€ä»¥è¦æ¸›ï¼Œè®“ç¸½åˆ†æ•¸è®Šå°
            except ValueError:
                pass # è§£æå¤±æ•—å‰‡ä½¿ç”¨é è¨­å€¼
        else: score += 100 # ç„¡æ—¥æœŸçš„ preview æ¯”æœ‰æ—¥æœŸçš„èˆŠ
    elif "-exp" in api_name_lower or "experimental" in api_name_lower: score = 2000
    else: # å˜—è©¦è§£ææ•¸å­—ç‰ˆæœ¬è™Ÿï¼Œä¾‹å¦‚ -001
        num_version_match = re.search(r'-(\d+)$', api_name_lower.split('/')[-1])
        if num_version_match:
            try:
                score = 3000 - int(num_version_match.group(1)) # æ•¸å­—è¶Šå¤§ç‰ˆæœ¬è¶Šæ–°ï¼Œæ‰€ä»¥æ¸›
            except ValueError:
                score = 3500
    return score

def sort_models_key_function(model_dict: Dict[str, Any]):
    api_name = model_dict.get("id", "")
    name_lower = api_name.lower()

    # å„ªå…ˆç´šçµ„ (ä¾†è‡ªé å®šç¾©è³‡æ–™æˆ–æ ¹æ“šåç¨±çŒœæ¸¬)
    priority_group = model_dict.get("sort_priority", 99) # é å®šç¾©çš„å„ªå…ˆç´š
    if priority_group == 99: # å¦‚æœæ²’æœ‰é å®šç¾©çš„ï¼Œå˜—è©¦çŒœæ¸¬
        if "gemini-1.5-pro" in name_lower: priority_group = 2 # æ¯” flash latest ç¨ä½
        elif "gemini-1.5-flash" in name_lower: priority_group = 3
        elif "gemini-pro" in name_lower and "vision" not in name_lower : priority_group = 12
        elif "gemini" in name_lower: priority_group = 15
        else: priority_group = 20

    version_score = get_model_version_score(name_lower)

    # ä¸»è¦ç‰ˆæœ¬è™Ÿ (ä¾‹å¦‚ 1.5, 1.0) ç”¨æ–¼æ¬¡ç´šæ’åº
    main_version_num = 0.0
    main_version_match = re.search(r'(gemini(?:-1\.5)?)-(\d+\.\d+|\d+)', name_lower) # gemini-1.5-pro, gemini-pro
    if not main_version_match: main_version_match = re.search(r'gemini-(\d+\.\d+|\d+)', name_lower)

    if main_version_match:
        try:
            main_version_num = float(main_version_match.groups()[-1]) # å–æœ€å¾Œæ•ç²çš„æ•¸å­—çµ„
        except ValueError:
            pass

    # æ’åºè¦å‰‡ï¼šå„ªå…ˆç´šçµ„ (è¶Šå°è¶Šå„ªå…ˆ) -> ä¸»è¦ç‰ˆæœ¬è™Ÿ (è¶Šå¤§è¶Šå„ªå…ˆï¼Œæ‰€ä»¥å–è² å€¼) -> ç‰ˆæœ¬åˆ†æ•¸ (è¶Šå°è¶Šå„ªå…ˆ) -> å­—æ¯é †åº
    return (priority_group, -main_version_num, version_score, name_lower)


@app.get("/api/get_models")
async def api_get_models_enhanced(api_key: str = Depends(get_validated_api_key)):
    # api_key parameter is now populated by the dependency, which also configures and validates genai
    logger.info("[API_GET_MODELS_ENHANCED] è«‹æ±‚ç²å–å¢å¼·å‹ AI æ¨¡å‹åˆ—è¡¨ã€‚é‡‘é‘°å·²é€éä¾è³´æ³¨å…¥é©—è­‰ã€‚")

    # The get_validated_api_key dependency will raise HTTPException if the key is invalid or not found,
    # so we don't need the explicit check here anymore. The code will only proceed if a valid key is available
    # and genai has been configured by the dependency.

    all_models_combined = {}
    try:
        logger.debug("[API_GET_MODELS_ENHANCED] æ­£åœ¨å¾ Google genai.list_models() æŸ¥è©¢ç·šä¸Šæ¨¡å‹...")
        online_models_count = 0
        for m_obj in genai.list_models():
            # ç¢ºä¿æ¨¡å‹æ”¯æ´ generateContent æ–¹æ³•
            if 'generateContent' in m_obj.supported_generation_methods:
                online_models_count +=1
                model_data = {
                    "id": m_obj.name,
                    "dropdown_display_name": m_obj.display_name if m_obj.display_name else m_obj.name.replace("models/", ""),
                    "chinese_display_name": m_obj.display_name if m_obj.display_name else m_obj.name.replace("models/", ""), # é è¨­ä½¿ç”¨ display_name
                    "chinese_summary_parenthesized": "", # é è¨­ç©º
                    "chinese_input_output": f"è¼¸å…¥ Tokens: {m_obj.input_token_limit}, è¼¸å‡º Tokens: {m_obj.output_token_limit}",
                    "chinese_suitable_for": "è«‹åƒè€ƒ API åŸå§‹æè¿°ã€‚",
                    "original_description_from_api": m_obj.description if m_obj.description else "N/A",
                    "sort_priority": 99 # é è¨­ä¸€å€‹è¼ƒä½çš„å„ªå…ˆç´šçµ¦ç·šä¸Šç²å–çš„ã€æœªåœ¨é å®šç¾©ä¸­å‡ºç¾çš„æ¨¡å‹
                }
                all_models_combined[m_obj.name] = model_data
        logger.debug(f"[API_GET_MODELS_ENHANCED] å¾ API ç²å–åˆ° {online_models_count} å€‹æ”¯æ´ generateContent çš„æ¨¡å‹ã€‚")

    except Exception as e_list_models:
        logger.error(f"[API_GET_MODELS_ENHANCED] å‘¼å« genai.list_models() å¤±æ•—: {e_list_models}")
        # å¦‚æœ API å‘¼å«å¤±æ•—ï¼Œè¿”å›æ˜ç¢ºçš„éŒ¯èª¤æ¨¡å‹ç‹€æ…‹çµ¦å‰ç«¯
        return JSONResponse(content=[
            {"id": "error-api-key-or-network",
             "dropdown_display_name": "éŒ¯èª¤ï¼šç„¡æ³•ç²å–æ¨¡å‹ (APIé‡‘é‘°æˆ–ç¶²è·¯å•é¡Œ)",
             "chinese_display_name": "APIé‡‘é‘°æˆ–ç¶²è·¯å•é¡Œ",
             "chinese_summary_parenthesized": "ï¼ˆç„¡æ³•é€£ç·šåˆ° Google APIï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ–é‡‘é‘°ç‹€æ…‹ï¼‰",
             "chinese_input_output": "N/A",
             "chinese_suitable_for": "è«‹æª¢æŸ¥æ‚¨çš„ API é‡‘é‘°æ˜¯å¦æ­£ç¢ºï¼Œä¸¦ç¢ºä¿ç¶²è·¯é€£æ¥æ­£å¸¸ã€‚",
             "original_description_from_api": "Failed to retrieve models from Google API. Check API key and network connectivity.",
             "sort_priority": -1 # æœ€é«˜å„ªå…ˆç´š
            }
        ], status_code=500) # è¿”å› 500 éŒ¯èª¤ç‹€æ…‹

    # å°‡é å®šç¾©çš„è³‡æ–™è¦†è“‹æˆ–æ·»åŠ åˆ°åˆä½µåˆ—è¡¨ä¸­ (é å®šç¾©çš„å„ªå…ˆ)
    for predefined_id, predefined_data in PREDEFINED_MODELS_DATA_APP.items():
        all_models_combined[predefined_id] = predefined_data # é å®šç¾©çš„è³‡æ–™æœ‰æ›´é«˜å„ªå…ˆç´š

    # è½‰æ›ç‚ºåˆ—è¡¨ä¸¦æ’åº
    sorted_models_list = sorted(all_models_combined.values(), key=sort_models_key_function)

    logger.info(f"[API_GET_MODELS_ENHANCED] è¿”å› {len(sorted_models_list)} å€‹æ¨¡å‹çµ¦å‰ç«¯ã€‚")
    return JSONResponse(content=sorted_models_list)


@app.post("/api/generate_report", status_code=202)
async def api_submit_generate_report_task(
    request_data: GenerateReportRequest,
    background_tasks: BackgroundTasks,
    api_key: str = Depends(get_validated_api_key) # Inject and validate API key
):
    task_id = str(uuid.uuid4())
    logger.info(f"[API_GEN_REPORT] [TASK {task_id}] æ”¶åˆ°å ±å‘Šç”Ÿæˆè«‹æ±‚: ä¾†æº='{request_data.source_path}', æ¨¡å‹='{request_data.model_id}'ã€‚APIé‡‘é‘°å·²æ³¨å…¥ã€‚")

    # Removed direct check of global_api_key and api_key_is_valid
    # if not global_api_key or not api_key_is_valid:
    #     logger.warning(f"[API_GEN_REPORT] [TASK {task_id}] API é‡‘é‘°ç„¡æ•ˆæˆ–æœªè¨­å®šã€‚")
    #     raise HTTPException(status_code=401, detail="ç„¡æ•ˆæˆ–æœªè¨­å®šçš„ API é‡‘é‘°ã€‚è«‹å…ˆè¨­å®šæœ‰æ•ˆçš„ API é‡‘é‘°ã€‚")

    if not os.path.exists(request_data.source_path):
        logger.error(f"[API_GEN_REPORT] [TASK {task_id}] éŸ³è¨Šä¾†æºæª”æ¡ˆä¸å­˜åœ¨: {request_data.source_path}")
        raise HTTPException(status_code=404, detail=f"æŒ‡å®šçš„éŸ³è¨Šä¾†æºæª”æ¡ˆä¸å­˜åœ¨: {os.path.basename(request_data.source_path)}")

    # å°‡ä»»å‹™è³‡è¨Šå­˜å…¥ SQLite
    try:
        request_data_json = json.dumps(request_data.model_dump()) # Pydantic v2
        download_links_json = json.dumps(None) # åˆå§‹åŒ– download_links ç‚º null JSON
        submit_time_iso = datetime.now(timezone.utc).isoformat()

        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(
            """
            INSERT INTO tasks (task_id, status, source_name, model_id, submit_time, request_data, download_links)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
            (task_id, "queued", os.path.basename(request_data.source_path), request_data.model_id,
             submit_time_iso, request_data_json, download_links_json)
        )
        conn.commit()
        logger.info(f"[API_GEN_REPORT] [TASK {task_id}] ä»»å‹™è³‡è¨ŠæˆåŠŸå¯«å…¥è³‡æ–™åº«ã€‚")
    except sqlite3.Error as e_sql:
        logger.error(f"[API_GEN_REPORT] [TASK {task_id}] [ERROR_DB] å°‡ä»»å‹™è³‡è¨Šå¯«å…¥ SQLite æ™‚ç™¼ç”ŸéŒ¯èª¤: {e_sql}")
        traceback.print_exc()
        # å³ä½¿è³‡æ–™åº«å¯«å…¥å¤±æ•—ï¼Œä¹Ÿå¯èƒ½å¸Œæœ›ä»»å‹™ç¹¼çºŒï¼ˆå¦‚æœè¨˜æ†¶é«”è™•ç†å¯è¡Œï¼‰ï¼Œæˆ–åœ¨æ­¤è™•æ‹‹å‡ºéŒ¯èª¤
        # ç‚ºäº†ä¿æŒèˆ‡ä¹‹å‰è¡Œç‚ºä¸€è‡´ï¼ˆè¨˜æ†¶é«”è™•ç†ï¼‰ï¼Œé€™è£¡æš«ä¸æ‹‹å‡º HTTP éŒ¯èª¤ï¼Œä½†è¨˜éŒ„åš´é‡éŒ¯èª¤
        # raise HTTPException(status_code=500, detail="å„²å­˜ä»»å‹™è³‡è¨Šåˆ°è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚")
        # ä¸éï¼Œå¦‚æœè³‡æ–™åº«æ˜¯ä¸»è¦ç‹€æ…‹ä¾†æºï¼Œå‰‡æ‡‰æ‹‹å‡ºéŒ¯èª¤ï¼š
        raise HTTPException(status_code=500, detail=f"ä»»å‹™æäº¤å¤±æ•—ï¼šç„¡æ³•å°‡ä»»å‹™è³‡è¨Šå„²å­˜åˆ°è³‡æ–™åº«ã€‚éŒ¯èª¤: {e_sql}")
    except Exception as e_gen:
        logger.error(f"[API_GEN_REPORT] [TASK {task_id}] [ERROR_UNEXPECTED] æº–å‚™ä»»å‹™è³‡æ–™æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e_gen}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ä»»å‹™æäº¤å¤±æ•—ï¼šæº–å‚™ä»»å‹™è³‡æ–™æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ã€‚éŒ¯èª¤: {e_gen}")
    finally:
        if 'conn' in locals() and conn:
            conn.close()

    # Pass the validated api_key to the background task
    background_tasks.add_task(process_audio_and_generate_report_task, task_id, request_data, api_key)
    logger.info(f"[API_GEN_REPORT] [TASK {task_id}] ä»»å‹™å·²åŠ å…¥ä½‡åˆ—ã€‚")
    return {"task_id": task_id, "message": "å ±å‘Šç”Ÿæˆä»»å‹™å·²åŠ å…¥ä½‡åˆ—ã€‚", "status": "queued"}


# --- ä»»å‹™ç‹€æ…‹æŸ¥è©¢ API ---
@app.get("/api/tasks")
async def get_all_tasks_status():
    logger.debug("[API_TASKS_ALL] è«‹æ±‚ç²å–æ‰€æœ‰ä»»å‹™çš„ç‹€æ…‹ã€‚")
    tasks_list = []
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM tasks ORDER BY submit_time DESC")
        rows = cursor.fetchall()
        conn.close()

        for row in rows:
            task_data = dict(row) # å°‡ sqlite3.Row è½‰æ›ç‚ºå­—å…¸
            # è§£æ JSON å­—ä¸²æ¬„ä½
            if task_data.get("download_links"):
                try:
                    task_data["download_links"] = json.loads(task_data["download_links"])
                except json.JSONDecodeError:
                    logger.warning(f"[API_TASKS_ALL] è§£æä»»å‹™ {task_data['task_id']} çš„ download_links JSON å¤±æ•—ã€‚")
                    task_data["download_links"] = None # æˆ–è¨­ç‚ºéŒ¯èª¤æç¤º
            # request_data é€šå¸¸ä¸éœ€è¦åœ¨åˆ—è¡¨è¦–åœ–ä¸­å®Œæ•´é¡¯ç¤ºï¼Œå¯ä»¥é¸æ“‡ä¸è§£ææˆ–åªè§£æéƒ¨åˆ†
            # if task_data.get("request_data"):
            #     try:
            #         task_data["request_data"] = json.loads(task_data["request_data"])
            #     except json.JSONDecodeError:
            #         logger.warning(f"[API_TASKS_ALL] è§£æä»»å‹™ {task_data['task_id']} çš„ request_data JSON å¤±æ•—ã€‚")
            #         task_data["request_data"] = None
            del task_data["request_data"] # å¾åˆ—è¡¨è¦–åœ–ä¸­ç§»é™¤è©³ç´°è«‹æ±‚è³‡æ–™ï¼Œä»¥æ¸›å°‘å‚³è¼¸é‡
            del task_data["result_preview_html"] # ä¹Ÿç§»é™¤ HTML é è¦½

            tasks_list.append(task_data)

        logger.info(f"[API_TASKS_ALL] æˆåŠŸå¾è³‡æ–™åº«æª¢ç´¢åˆ° {len(tasks_list)} å€‹ä»»å‹™ã€‚")

    except sqlite3.Error as e_sql:
        logger.error(f"[API_TASKS_ALL] [ERROR_DB] å¾ SQLite è®€å–æ‰€æœ‰ä»»å‹™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e_sql}")
        traceback.print_exc()
        # è¿”å›ç©ºåˆ—è¡¨æˆ–éŒ¯èª¤è¨Šæ¯ï¼Œå–æ±ºæ–¼æœŸæœ›çš„è¡Œç‚º
        # return JSONResponse(content={"error": "ç„¡æ³•ç²å–ä»»å‹™åˆ—è¡¨", "detail": str(e_sql)}, status_code=500)
        # ç‚ºäº†å‰ç«¯ç›¸å®¹æ€§ï¼Œæš«æ™‚è¿”å›ç©ºåˆ—è¡¨
    except Exception as e_gen:
        logger.error(f"[API_TASKS_ALL] [ERROR_UNEXPECTED] è™•ç†æ‰€æœ‰ä»»å‹™ç‹€æ…‹æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e_gen}")
        traceback.print_exc()
        # return JSONResponse(content={"error": "è™•ç†ä»»å‹™åˆ—è¡¨æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤", "detail": str(e_gen)}, status_code=500)

    return JSONResponse(content=tasks_list) # å·²æŒ‰ submit_time DESC æ’åº

@app.get("/api/tasks/{task_id}")
async def get_task_status_and_result(task_id: str):
    logger.debug(f"[API_TASK_ID] è«‹æ±‚ç²å–ä»»å‹™ {task_id} çš„è©³ç´°ç‹€æ…‹ã€‚")
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM tasks WHERE task_id = ?", (task_id,))
        row = cursor.fetchone()
        conn.close()

        if not row:
            logger.warning(f"[API_TASK_ID] åœ¨è³‡æ–™åº«ä¸­æ‰¾ä¸åˆ°ä»»å‹™ ID: {task_id}")
            raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æŒ‡å®šçš„ä»»å‹™ IDã€‚")

        task_data = dict(row) # å°‡ sqlite3.Row è½‰æ›ç‚ºå­—å…¸

        # è§£æ JSON å­—ä¸²æ¬„ä½
        if task_data.get("download_links"):
            try:
                task_data["download_links"] = json.loads(task_data["download_links"])
            except json.JSONDecodeError:
                logger.warning(f"[API_TASK_ID] è§£æä»»å‹™ {task_id} çš„ download_links JSON å¤±æ•—ã€‚")
                task_data["download_links"] = {"error": "Failed to parse download links"}

        # request_data é€šå¸¸ä¸éœ€è¦åœ¨å–®ä»»å‹™è©³ç´°è¦–åœ–ä¸­è¿”å›çµ¦å®¢æˆ¶ç«¯ï¼Œé™¤éç‰¹å®šéœ€æ±‚
        if 'request_data' in task_data:
            del task_data['request_data'] # é€šå¸¸ä¸è¿”å›å®Œæ•´çš„åŸå§‹è«‹æ±‚

        logger.info(f"[API_TASK_ID] æˆåŠŸå¾è³‡æ–™åº«æª¢ç´¢åˆ°ä»»å‹™ {task_id} çš„è©³ç´°è³‡è¨Šã€‚")
        return JSONResponse(content=task_data)

    except sqlite3.Error as e_sql:
        logger.error(f"[API_TASK_ID] [ERROR_DB] å¾ SQLite è®€å–ä»»å‹™ {task_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e_sql}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æŸ¥è©¢ä»»å‹™ç‹€æ…‹æ™‚ç™¼ç”Ÿè³‡æ–™åº«éŒ¯èª¤: {e_sql}")
    except Exception as e_gen:
        logger.error(f"[API_TASK_ID] [ERROR_UNEXPECTED] è™•ç†ä»»å‹™ {task_id} ç‹€æ…‹æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e_gen}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æŸ¥è©¢ä»»å‹™ç‹€æ…‹æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e_gen}")


# --- ä¸‹è¼‰ç”Ÿæˆçš„å ±å‘Šæª”æ¡ˆ API ---
try:
    if os.path.exists(GENERATED_REPORTS_DIR):
        app.mount("/generated_reports", StaticFiles(directory=GENERATED_REPORTS_DIR), name="generated_reports")
        logger.info(f"å·²æ›è¼‰ '{GENERATED_REPORTS_DIR}' è‡³ '/generated_reports' ä»¥ä¾›ç›´æ¥ä¸‹è¼‰ã€‚")
except Exception as e:
    logger.error(f"æ›è¼‰ generated_reports ç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

# --- ä¸»é é¢ ---
@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    logger.debug("[API_ROOT] è«‹æ±‚ä¸»é é¢ã€‚")
    if templates is None:
        error_msg = "<html><body><h1>éŒ¯èª¤ï¼šä¸»æ¨¡æ¿å¼•æ“æœªåˆå§‹åŒ–ã€‚è«‹æª¢æŸ¥ä¼ºæœå™¨æ—¥èªŒã€‚</h1></body></html>"
        logger.critical("ä¸»æ¨¡æ¿å¼•æ“æœªåˆå§‹åŒ–ã€‚")
        return HTMLResponse(error_msg, status_code=500)
    return templates.TemplateResponse("index.html", {"request": request, "title": "AI_paper æ™ºèƒ½åŠ©ç† v2.4 (ç©©å®šç‰ˆ)"})


# --- API ç‹€æ…‹ ---
@app.get("/api/status")
async def get_api_status():
    logger.debug("[API_STATUS] è«‹æ±‚ API ç‹€æ…‹ã€‚")
    return {"status": "AI_paper API v2.4 is running", "version": "2.4.0_optimized"}

if __name__ == "__main__":
    logger.info("è‹¥åœ¨æœ¬åœ°åŸ·è¡Œ app.pyï¼Œè«‹ç¢ºä¿ CWD è¨­å®šæ­£ç¢ºã€‚")
    uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True, workers=1)

